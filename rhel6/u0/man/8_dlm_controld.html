<!-- Creator     : groff version 1.18.1.4 -->
<!-- CreationDate: Sat Nov 12 21:57:39 2016 -->
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta name="Content-Style" content="text/css">
<title></title>
</head>
<body>

<hr>

<p>DLM_CONTROLD(8) cluster DLM_CONTROLD(8)</p>

<p>NAME dlm_controld - daemon that configures dlm according
to cluster events</p>

<p>SYNOPSIS dlm_controld [OPTIONS]</p>

<p>DESCRIPTION The dlm lives in the kernel, and the cluster
infrastructure (corosync membership and group management)
lives in user space. The dlm in the kernel needs to
adjust/recover for certain cluster events. Its the job of
dlm_controld to receive these events and reconfigure the
kernel dlm as needed. dlm_controld controls and configures
the dlm through sysfs and configfs files that are considered
dlm-internal interfaces.</p>

<p>The cman init script usually starts the dlm_controld
daemon.</p>

<p>OPTIONS Command line options override a corresponding
setting in cluster.conf.</p>

<p>-D Enable debugging to stderr and dont fork. See also
dlm_tool dump in dlm_tool(8).</p>

<p>-L Enable debugging to log file. See also logging in
cluster.conf(5).</p>

<p>-K Enable kernel dlm debugging messages. See also
log_debug below.</p>

<p>-r num dlm kernel lowcomms protocol, 0 tcp, 1 sctp, 2
detect. 2 selects tcp if corosync rrp_mode is
&quot;none&quot;, otherwise sctp. Default 2.</p>

<p>-g num groupd compatibility mode, 0 off, 1 on. Default
0.</p>

<p>-f num Enable (1) or disable (0) fencing recovery
dependency. Default 1.</p>

<p>-q num Enable (1) or disable (0) quorum recovery
dependency. Default 0.</p>

<p>-d num Enable (1) or disable (0) deadlock detection
code. Default 0.</p>

<p>-p num Enable (1) or disable (0) plock code for cluster
fs. Default 1.</p>

<p>-l num Limit the rate of plock operations, 0 for no
limit. Default 0.</p>

<p>-o num Enable (1) or disable (0) plock ownership.
Default 0.</p>

<p>-t ms Plock ownership drop resources time
(milliseconds). Default 10000.</p>

<p>-c num Plock ownership drop resources count. Default
10.</p>

<p>-a ms Plock ownership drop resources age (milliseconds).
Default 10000.</p>

<p>-P Enable plock debugging messages (can produce
excessive output).</p>

<p>-h Print a help message describing available options,
then exit.</p>

<p>-V Print program version information, then exit.</p>

<p>FILES cluster.conf(5) is usually located at
/etc/cluster/cluster.conf. It is not read directly. Other
cluster components load the contents into memory, and the
values are accessed through the libccs library.</p>

<p>Configuration options for dlm (kernel) and dlm_controld
are added to the &lt;dlm /&gt; section of cluster.conf,
within the top level &lt;cluster&gt; section.</p>

<p>Kernel options protocol The network protocol can be set
to tcp, sctp or detect which selects tcp or sctp based on
the corosync rrp_mode configuration (redundant ring
protocol). The rrp_mode &quot;none&quot; results in tcp.
Default detect.</p>

<p>&lt;dlm protocol=&quot;detect&quot;/&gt;</p>

<p>timewarn After waiting timewarn centiseconds, the dlm
will emit a warning via netlink. This only applies to
lockspaces created with the DLM_LSFL_TIMEWARN flag, and is
used for deadlock detection. Default 500 (5 seconds).</p>

<p>&lt;dlm timewarn=&quot;500&quot;/&gt;</p>

<p>log_debug DLM kernel debug messages can be enabled by
setting log_debug to 1. Default 0.</p>

<p>&lt;dlm log_debug=&quot;0&quot;/&gt;</p>

<p>clusternode/weight The lock directory weight can be
specified one the clusternode lines. Weights would usually
be used in the lock server configurations shown below
instead.</p>

<p>&lt;clusternode name=&quot;node01&quot;
nodeid=&quot;1&quot; weight=&quot;1&quot;/&gt;</p>

<p>Daemon options enable_fencing See command line
description.</p>

<p>&lt;dlm enable_fencing=&quot;1&quot;/&gt;</p>

<p>enable_quorum See command line description.</p>

<p>&lt;dlm enable_quorum=&quot;0&quot;/&gt;</p>

<p>enable_deadlk See command line description.</p>

<p>&lt;dlm enable_deadlk=&quot;0&quot;/&gt;</p>

<p>enable_plock See command line description.</p>

<p>&lt;dlm enable_plock=&quot;1&quot;/&gt;</p>

<p>plock_rate_limit See command line description.</p>

<p>&lt;dlm plock_rate_limit=&quot;0&quot;/&gt;</p>

<p>plock_ownership See command line description.</p>

<p>&lt;dlm plock_ownership=&quot;0&quot;/&gt;</p>

<p>drop_resources_time See command line description.</p>

<p>&lt;dlm drop_resources_time=&quot;10000&quot;/&gt;</p>

<p>drop_resources_count See command line description.</p>

<p>&lt;dlm drop_resources_count=&quot;10&quot;/&gt;</p>

<p>drop_resources_age See command line description.</p>

<p>&lt;dlm drop_resources_age=&quot;10000&quot;/&gt;</p>

<p>plock_debug Enable (1) or disable (0) plock debugging
messages (can produce excessive output). Default 0.</p>

<p>&lt;dlm plock_debug=&quot;0&quot;/&gt;</p>

<p>Disabling resource directory Lockspaces usually use a
resource directory to keep track of which node is the master
of each resource. The dlm can operate without the resource
directory, though, by statically assigning the master of a
resource using a hash of the resource name. To enable, set
the per- lockspace nodir option to 1.</p>

<p>&lt;dlm&gt; &lt;lockspace name=&quot;foo&quot;
nodir=&quot;1&quot;&gt; &lt;/dlm&gt;</p>

<p>Lock-server configuration The nodir setting can be
combined with node weights to create a config- uration where
select node(s) are the master of all resources/locks. These
master nodes can be viewed as &quot;lock servers&quot; for
the other nodes.</p>

<p>&lt;dlm&gt; &lt;lockspace name=&quot;foo&quot;
nodir=&quot;1&quot;&gt; &lt;master
name=&quot;node01&quot;/&gt; &lt;/lockspace&gt;
&lt;/dlm&gt;</p>

<p>or,</p>

<p>&lt;dlm&gt; &lt;lockspace name=&quot;foo&quot;
nodir=&quot;1&quot;&gt; &lt;master
name=&quot;node01&quot;/&gt; &lt;master
name=&quot;node02&quot;/&gt; &lt;/lockspace&gt;
&lt;/dlm&gt;</p>

<p>Lock management will be partitioned among the available
masters. There can be any number of masters defined. The
designated master nodes will master all resources/locks
(according to the resource name hash). When no masters are
members of the lockspace, then the nodes revert to the
common fully-distributed configuration. Recovery is faster,
with lit- tle disruption, when a non-master node
joins/leaves.</p>

<p>There is no special mode in the dlm for this lock server
configuration, its just a natural consequence of combining
the &quot;nodir&quot; option with node weights. When a
lockspace has master nodes defined, the master has a default
weight of 1 and all non-master nodes have weight of 0. An
explicit non-zero weight can also be assigned to master
nodes, e.g.</p>

<p>&lt;dlm&gt; &lt;lockspace name=&quot;foo&quot;
nodir=&quot;1&quot;&gt; &lt;master name=&quot;node01&quot;
weight=&quot;2&quot;/&gt; &lt;master name=&quot;node02&quot;
weight=&quot;1&quot;/&gt; &lt;/lockspace&gt;
&lt;/dlm&gt;</p>

<p>In which case node01 will master 2/3 of the total
resources and node2 will master the other 1/3.</p>

<p>SEE ALSO dlm_tool(8), fenced(8), cman(5),
cluster.conf(5)</p>

<p>cluster 2009-01-18 DLM_CONTROLD(8)</p>
<hr>
</body>
</html>

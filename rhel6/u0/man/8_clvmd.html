<!-- Creator     : groff version 1.18.1.4 -->
<!-- CreationDate: Sat Nov 12 21:55:55 2016 -->
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta name="Content-Style" content="text/css">
<title></title>
</head>
<body>

<hr>

<p>CLVMD(8) CLVMD(8)</p>

<p>NAME clvmd - cluster LVM daemon</p>

<p>SYNOPSIS clvmd [-d [&lt;value&gt;]] [-C] [-h] [-R] [-S]
[-t &lt;timeout&gt;] [-T &lt;start timeout&gt;] [-V]</p>

<p>DESCRIPTION clvmd is the daemon that distributes LVM
metadata updates around a cluster. It must be running on all
nodes in the cluster and will give an error if a node in the
cluster does not have this daemon running.</p>

<p>OPTIONS -d [&lt;value&gt;] Enable debug logging. Value
can be 0, 1 or 2. 0 disables debug logging in a running
clvmd 1 sends debug logs to stderr (clvmd will not fork in
this case) 2 sends debug logs to syslog If -d is specified
without a value then 1 is assumed if you are starting a new
clvmd, 2 if you are enabling debug in a running clvmd.</p>

<p>-C Only valid if -d is also specified. Tells all clvmds
in a clus- ter to enable/disable debug logging. Without this
switch, only the local clvmd will change its debug level to
that given with -d. This does not work correctly if
specified on the command-line that starts clvmd. If you want
to start clvmd and enable clus- ter-wide logging then the
command needs to be issued twice, eg: clvmd clvmd -d2</p>

<p>-t &lt;timeout&gt; Specifies the timeout for commands to
run around the cluster. This should not be so small that
commands with many disk updates to do will fail, so you may
need to increase this on systems with very large disk farms.
The default is 30 seconds.</p>

<p>-T &lt;start timeout&gt; Specifies the timeout for clvmd
daemon startup. If the daemon does not report that it has
started up within this time then the parent command will
exit with status of 5. This does NOT mean that clvmd has not
started! What it means is that the startup of clvmd has been
delayed for some reason; the most likely cause of this is an
inquorate cluster though it could be due to locking
latencies on a cluster with large numbers of logical
volumes. If you get the return code of 5 it is usually not
necessary to restart clvmd - it will start as soon as that
blockage has cleared. This flag is to allow startup scripts
to exit in a timely fashion even if the cluster is stalled
for some reason. The default is 0 (no timeout) and the value
is in seconds. Dont set this too small or you will
experience spurious errors. 10 or 20 seconds might be
sensible. This timeout will be ignored if you start clvmd
with the -d switch.</p>

<p>-R Tells all the running clvmds in the cluster to reload
their device cache and re-read the lvm configuration file.
This com- mand should be run whenever the devices on a
cluster system are changed.</p>

<p>-S Tells the running clvmd to exit and reexecute itself,
for exam- ple at the end of a package upgrade. The new
instance is instructed to reacquire any locks in the same
state as they were previously held. (Alternative methods of
restarting the daemon have the side effect of changing
exclusive LV locks into shared locks.)</p>

<p>-I Selects the cluster manager to use for locking and
internal com- munications, the available managers will be
listed as part of the clvmd -h output. clvmd will use the
first cluster manager that succeeds, and it checks them in
the order cman,gulm,corosync,openais. As it is quite
possible to have (eg) corosync and cman available on the
same system you might have to manually specify this option
to override the search.</p>

<p>-V Display the version of the cluster LVM daemon.</p>

<p>SEE ALSO lvm(8)</p>

<p>Red Hat Inc LVM TOOLS 2.02.72(2) (2010-07-28)
CLVMD(8)</p>
<hr>
</body>
</html>

<!-- Creator     : groff version 1.22.2 -->
<!-- CreationDate: Fri Nov 11 22:52:55 2016 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title>WWW::RobotRules</title>

</head>
<body>

<h1 align="center">WWW::RobotRules</h1>

<a href="#NAME">NAME</a><br>
<a href="#SYNOPSIS">SYNOPSIS</a><br>
<a href="#DESCRIPTION">DESCRIPTION</a><br>
<a href="#ROBOTS.TXT">ROBOTS.TXT</a><br>
<a href="#ROBOTS.TXT EXAMPLES">ROBOTS.TXT EXAMPLES</a><br>
<a href="#SEE ALSO">SEE ALSO</a><br>
<a href="#COPYRIGHT">COPYRIGHT</a><br>

<hr>


<h2>NAME
<a name="NAME"></a>
</h2>



<p style="margin-left:11%; margin-top: 1em">WWW::RobotRules
&minus; database of robots.txt&minus;derived permissions</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>


<pre style="margin-left:11%; margin-top: 1em"> use WWW::RobotRules;
 my $rules = WWW::RobotRules&minus;&gt;new('MOMspider/1.0');
 use LWP::Simple qw(get);
 {
   my $url = &quot;http://some.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules&minus;&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 {
   my $url = &quot;http://some.other.place/robots.txt&quot;;
   my $robots_txt = get $url;
   $rules&minus;&gt;parse($url, $robots_txt) if defined $robots_txt;
 }
 # Now we can check if a URL is valid for those servers
 # whose &quot;robots.txt&quot; files we've gotten and parsed:
 if($rules&minus;&gt;allowed($url)) {
     $c = get $url;
     ...
 }</pre>


<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">This module
parses <i>/robots.txt</i> files as specified in &quot;A
Standard for Robot Exclusion&quot;, at
&lt;http://www.robotstxt.org/wc/norobots.html&gt; Webmasters
can use the <i>/robots.txt</i> file to forbid conforming
robots from accessing parts of their web site.</p>

<p style="margin-left:11%; margin-top: 1em">The parsed
files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given
<small>URL</small> is prohibited. The same WWW::RobotRules
object can be used for one or more parsed <i>/robots.txt</i>
files on any number of hosts.</p>

<p style="margin-left:11%; margin-top: 1em">The following
methods are provided: <br>
$rules = WWW::RobotRules&minus;&gt;new($robot_name)</p>

<p style="margin-left:17%;">This is the constructor for
WWW::RobotRules objects. The first argument given to
<i>new()</i> is the name of the robot.</p>


<p style="margin-left:11%;">$rules&minus;&gt;parse($robot_txt_url,
$content, $fresh_until)</p>

<p style="margin-left:17%;">The <i>parse()</i> method takes
as arguments the <small>URL</small> that was used to
retrieve the <i>/robots.txt</i> file, and the contents of
the file.</p>


<p style="margin-left:11%;">$rules&minus;&gt;allowed($uri)</p>

<p style="margin-left:17%;">Returns <small>TRUE</small> if
this robot is allowed to retrieve this
<small>URL.</small></p>


<p style="margin-left:11%;">$rules&minus;&gt;agent([$name])</p>

<p style="margin-left:17%;">Get/set the agent name.
<small>NOTE:</small> Changing the agent name will clear the
robots.txt rules and expire times out of the cache.</p>

<h2>ROBOTS.TXT
<a name="ROBOTS.TXT"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">The format and
semantics of the &quot;/robots.txt&quot; file are as follows
(this is an edited abstract of
&lt;http://www.robotstxt.org/wc/norobots.html&gt;):</p>

<p style="margin-left:11%; margin-top: 1em">The file
consists of one or more records separated by one or more
blank lines. Each record contains lines of the form</p>

<pre style="margin-left:11%; margin-top: 1em">  &lt;field&minus;name&gt;: &lt;value&gt;</pre>


<p style="margin-left:11%; margin-top: 1em">The field name
is case insensitive. Text after the &rsquo;#&rsquo;
character on a line is ignored during parsing. This is used
for comments. The following &lt;field&minus;names&gt; can be
used: <br>
User-Agent</p>

<p style="margin-left:15%;">The value of this field is the
name of the robot the record is describing access policy
for. If more than one <i>User-Agent</i> field is present the
record describes an identical access policy for more than
one robot. At least one field needs to be present per
record. If the value is &rsquo;*&rsquo;, the record
describes the default access policy for any robot that has
not not matched any of the other records.</p>

<p style="margin-left:15%; margin-top: 1em">The
<i>User-Agent</i> fields must occur before the
<i>Disallow</i> fields. If a record contains a
<i>User-Agent</i> field after a <i>Disallow</i> field, that
constitutes a malformed record. This parser will assume that
a blank line should have been placed before that
<i>User-Agent</i> field, and will break the record into two.
All the fields before the <i>User-Agent</i> field will
constitute a record, and the <i>User-Agent</i> field will be
the first field in a new record.</p>

<p style="margin-left:11%;">Disallow</p>

<p style="margin-left:15%;">The value of this field
specifies a partial <small>URL</small> that is not to be
visited. This can be a full path, or a partial path; any
<small>URL</small> that starts with this value will not be
retrieved</p>

<p style="margin-left:11%; margin-top: 1em">Unrecognized
records are ignored.</p>

<h2>ROBOTS.TXT EXAMPLES
<a name="ROBOTS.TXT EXAMPLES"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">The following
example &quot;/robots.txt&quot; file specifies that no
robots should visit any <small>URL</small> starting with
&quot;/cyberworld/map/&quot; or &quot;/tmp/&quot;:</p>

<pre style="margin-left:11%; margin-top: 1em">  User&minus;agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  Disallow: /tmp/ # these will soon disappear</pre>


<p style="margin-left:11%; margin-top: 1em">This example
&quot;/robots.txt&quot; file specifies that no robots should
visit any <small>URL</small> starting with
&quot;/cyberworld/map/&quot;, except the robot called
&quot;cybermapper&quot;:</p>

<pre style="margin-left:11%; margin-top: 1em">  User&minus;agent: *
  Disallow: /cyberworld/map/ # This is an infinite virtual URL space
  # Cybermapper knows where to go.
  User&minus;agent: cybermapper
  Disallow:</pre>


<p style="margin-left:11%; margin-top: 1em">This example
indicates that no robots should visit this site further:</p>

<pre style="margin-left:11%; margin-top: 1em">  # go away
  User&minus;agent: *
  Disallow: /</pre>


<p style="margin-left:11%; margin-top: 1em">This is an
example of a malformed robots.txt file.</p>

<pre style="margin-left:11%; margin-top: 1em">  # robots.txt for ancientcastle.example.com
  # I've locked myself away.
  User&minus;agent: *
  Disallow: /
  # The castle is your home now, so you can go anywhere you like.
  User&minus;agent: Belle
  Disallow: /west&minus;wing/ # except the west wing!
  # It's good to be the Prince...
  User&minus;agent: Beast
  Disallow:</pre>


<p style="margin-left:11%; margin-top: 1em">This file is
missing the required blank lines between records. However,
the intention is clear.</p>

<h2>SEE ALSO
<a name="SEE ALSO"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">LWP::RobotUA,
WWW::RobotRules::AnyDBM_File</p>

<h2>COPYRIGHT
<a name="COPYRIGHT"></a>
</h2>


<pre style="margin-left:11%; margin-top: 1em">  Copyright 1995&minus;2009, Gisle Aas
  Copyright 1995, Martijn Koster</pre>


<p style="margin-left:11%; margin-top: 1em">This library is
free software; you can redistribute it and/or modify it
under the same terms as Perl itself.</p>
<hr>
</body>
</html>

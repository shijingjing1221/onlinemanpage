<!-- Creator     : groff version 1.22.2 -->
<!-- CreationDate: Fri Nov 11 01:47:40 2016 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title>HUGEADM</title>

</head>
<body>

<h1 align="center">HUGEADM</h1>

<a href="#NAME">NAME</a><br>
<a href="#SYNOPSIS">SYNOPSIS</a><br>
<a href="#DESCRIPTION">DESCRIPTION</a><br>
<a href="#SEE ALSO">SEE ALSO</a><br>
<a href="#AUTHORS">AUTHORS</a><br>

<hr>


<h2>NAME
<a name="NAME"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">hugeadm &minus;
Configure the system huge page pools</p>

<h2>SYNOPSIS
<a name="SYNOPSIS"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em"><b>hugeadm
[options]</b></p>

<h2>DESCRIPTION
<a name="DESCRIPTION"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em"><b>hugeadm</b>
displays and configures the systems huge page pools. The
size of the pools is set as a minimum and maximum threshold.
The minimum value is allocated up front by the kernel and
guaranteed to remain as hugepages until the pool is shrunk.
If a maximum is set, the system will dynamically allocate
pages if applications request more hugepages than the
minimum size of the pool. There is no guarantee that more
pages than this minimum pool size can be allocated.</p>

<p style="margin-left:11%; margin-top: 1em">The following
options create mounts hugetlbfs mount points. <b><br>
--create-mounts</b></p>

<p style="margin-left:22%; margin-top: 1em">This creates
mount points for each supported huge page size under
/var/lib/hugetlbfs. After creation they are mounts and are
owned by root:root with permissions set to 770. Each mount
point is named pagesize-&lt;size in bytes&gt;.</p>


<p style="margin-left:11%;"><b>--create-user-mounts=&lt;user&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">This creates
mount points for each supported huge page size under
/var/lib/hugetlbfs/user/&lt;user&gt;. Mount point naming is
the same as --create-mounts. After creation they are mounted
and are owned by &lt;user&gt;:root with permissions set to
700.</p>


<p style="margin-left:11%;"><b>--create-group-mounts=&lt;group&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">This creates
mount points for each supported huge page size under
/var/lib/hugetlbfs/group/&lt;group&gt;. Mount point naming
is the same as --create-mounts. After creation they are
mounted and are owned by root:&lt;group&gt; with permissions
set to 070.</p>


<p style="margin-left:11%;"><b>--create-global-mounts</b></p>

<p style="margin-left:22%; margin-top: 1em">This creates
mount points for each supported huge page size under
/var/lib/hugetlbfs/global. Mount point naming is the same as
--create-mounts. After creation they are mounted and are
owned by root:root with permissions set to 1777.</p>

<p style="margin-left:22%; margin-top: 1em">The following
options affect how mount points are created.</p>

<p style="margin-left:11%;"><b>--max-size</b></p>

<p style="margin-left:22%; margin-top: 1em">This option is
used in conjunction with --create-*-mounts. It limits the
maximum amount of memory used by files within the mount
point rounded up to the nearest huge page size. This can be
used for example to grant different huge page quotas to
individual users or groups.</p>

<p style="margin-left:11%;"><b>--max-inodes</b></p>

<p style="margin-left:22%; margin-top: 1em">This option is
used in conjunction with --create-*-mounts. It limits the
number of inodes (e.g. files) that can be created on the new
mount points. This limits the number of mappings that can be
created on a mount point. It could be used for example to
limit the number of application instances that used a mount
point as long as it was known how many inodes each
application instance required.</p>

<p style="margin-left:22%; margin-top: 1em">The following
options display information about the pools.</p>

<p style="margin-left:11%;"><b>--pool-list</b></p>

<p style="margin-left:22%; margin-top: 1em">This displays
the Minimum, Current and Maximum number of huge pages in the
pool for each pagesize supported by the system. The
&quot;Minimum&quot; value is the size of the static pool and
there will always be at least this number of hugepages in
use by the system, either by applications or kept by the
kernel in a reserved pool. The &quot;Current&quot; value is
the number of hugepages currently in use, either by
applications or stored on the kernels free list. The
&quot;Maximum&quot; value is the largest number of hugepages
that can be in use at any given time.</p>


<p style="margin-left:11%;"><b>--set-recommended-min_free_kbytes</b></p>

<p style="margin-left:22%; margin-top: 1em">Fragmentation
avoidance in the kernel depends on avoiding pages of
different mobility types being mixed with a pageblock arena
- typically the size of the default huge page size. The more
mixing that occurs, the less likely the huge page pool will
be able to dynamically resize. The easiest means of avoiding
mixing is to increase /proc/sys/vm/min_free_kbytes. This
parameter sets min_free_kbytes to a recommended value to aid
fragmentation avoidance.</p>


<p style="margin-left:11%;"><b>--set-recommended-shmmax</b></p>

<p style="margin-left:22%; margin-top: 1em">The maximum
shared memory segment size should be set to at least the
size of the largest shared memory segment size you want
available for applications using huge pages, via
/proc/sys/kernel/shmmax. Optionally, it can be set
automatically to match the maximum possible size of all huge
page allocations and thus the maximum possible shared memory
segment size, using this switch.</p>


<p style="margin-left:11%;"><b>--set-shm-group=&lt;gid|groupname&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">Users in the
group specified in /proc/sys/vm/hugetlb_shm_group are
granted full access to huge pages. The sysctl takes a
numeric gid, but this hugeadm option can set it for you,
using either a gid or group name.</p>

<p style="margin-left:11%;"><b>--page-sizes</b></p>

<p style="margin-left:22%; margin-top: 1em">This displays
every page size supported by the system and has a pool
configured.</p>

<p style="margin-left:11%;"><b>--page-sizes-all</b></p>

<p style="margin-left:22%; margin-top: 1em">This displays
all page sizes supported by the system, even if no pool is
available.</p>

<p style="margin-left:11%;"><b>--list-all-mounts</b></p>

<p style="margin-left:22%; margin-top: 1em">This displays
all active mount points for hugetlbfs.</p>

<p style="margin-left:11%; margin-top: 1em">The following
options configure the pool. <b><br>

--pool-pages-min=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">This option
sets or adjusts the Minimum number of hugepages in the pool
for pagesize <b>size</b>. <b>size</b> may be specified in
bytes or in kilobytes, megabytes, or gigabytes by appending
K, M, or G respectively, or as DEFAULT, which uses the
system&rsquo;s default huge page size for <b>size</b>. The
pool size adjustment can be specified by <b>pagecount</b>
pages or by <b>memsize</b>, if postfixed with G, M, or K,
for gigabytes, megabytes, or kilobytes, respectively. If the
adjustment is specified via <b>memsize</b>, then the
<b>pagecount</b> will be calculated for you, based on page
size <b>size</b>. The pool is set to <b>pagecount</b> pages
if + or - are not specified. If + or - are specified, then
the size of the pool will adjust by that amount. Note that
there is no guarantee that the system can allocate the
hugepages requested for the Minimum pool. The size of the
pools should be checked after executing this command to
ensure they were successful.</p>

<p style="margin-left:11%;"><b>--obey-numa-mempol</b></p>

<p style="margin-left:22%; margin-top: 1em">This option
requests that allocation of huge pages to the static pool
with <b>--pool-pages-min</b> obey the NUMA memory policy of
the current process. This policy can be explicitly specified
using numactl or inherited from a parent process.</p>


<p style="margin-left:11%;"><b>--pool-pages-max=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">This option
sets or adjusts the Maximum number of hugepages. Note that
while the Minimum number of pages are guaranteed to be
available to applications, there is not guarantee that the
system can allocate the pages on demand when the number of
huge pages requested by applications is between the Minimum
and Maximum pool sizes. See --pool-pages-min for usage
syntax.</p>


<p style="margin-left:11%;"><b>--enable-zone-movable</b></p>

<p style="margin-left:22%; margin-top: 1em">This option
enables the use of the MOVABLE zone for the allocation of
hugepages. This zone is created when kernelcore= or
movablecore= are specified on the kernel command line but
the zone is not used for the allocation of huge pages by
default as the intended use for the zone may be to guarantee
that memory can be off-lined and hot-removed. The kernel
guarantees that the pages within this zone can be reclaimed
unlike some kernel buffers for example. Unless pages are
locked with mlock(), the hugepage pool can grow to at least
the size of the movable zone once this option is set. Use
sysctl to permanently enable the use of the MOVABLE zone for
the allocation of huge pages.</p>


<p style="margin-left:11%;"><b>--disable-zone-movable</b></p>

<p style="margin-left:22%; margin-top: 1em">This option
disables the use of the MOVABLE zone for the future
allocation of huge pages. Note that existing huge pages are
not reclaimed from the zone. Use sysctl to permanently
disable the use of the MOVABLE zone for the allocation of
huge pages.</p>

<table width="100%" border="0" rules="none" frame="void"
       cellspacing="0" cellpadding="0">
<tr valign="top" align="left">
<td width="11%"></td>
<td width="9%">


<p><b>--hard</b></p></td>
<td width="2%"></td>
<td width="78%">


<p>This option is specified with --pool-pages-min to retry
allocations multiple times on failure to allocate the
desired count of pages. It initially tries to resize the
pool up to 5 times and continues to try if progress is being
made towards the resize.</p></td></tr>
</table>


<p style="margin-left:11%;"><b>--add-temp-swap&lt;=count&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">This options is
specified with --pool-pages-min to initialize a temporary
swap file for the duration of the pool resize. When
increasing the size of the pool, it can be necessary to
reclaim pages so that contiguous memory is freed and this
often requires swap to be successful. Swap is only created
for a positive resize, and is then removed once the resize
operation is completed. The default swap size is 5 huge
pages, the optional argument &lt;count&gt; sets the swap
size to &lt;count&gt; huge pages.</p>

<p style="margin-left:11%;"><b>--add-ramdisk-swap</b></p>

<p style="margin-left:22%; margin-top: 1em">This option is
specified with --pool-pages-min to initialize swap in memory
on ram disks. When increasing the size of the pool, it can
be necessary to reclaim pages so that contiguous memory is
freed and this often requires swap to be successful. If
there isn&rsquo;t enough free disk space, swap can be
initialized in RAM using this option. If the size of one
ramdisk is not greater than the huge page size, then swap is
initialized on multiple ramdisks. Swap is only created for a
positive resize, and by default is removed once the resize
operation is completed.</p>

<p style="margin-left:11%;"><b>--persist</b></p>

<p style="margin-left:22%; margin-top: 1em">This option is
specified with the --add-temp-swap or --add-ramdisk-swap to
make the swap space persist after the resize operation is
completed. The swap spaces can later be removed manually
using the swapoff command.</p>

<p style="margin-left:11%; margin-top: 1em">The following
options tune the transparent huge page usage <b><br>
--thp-always</b></p>

<p style="margin-left:22%; margin-top: 1em">Enable
transparent huge pages always</p>

<p style="margin-left:11%;"><b>--thp-madvise</b></p>

<p style="margin-left:22%; margin-top: 1em">Enable
transparent huge pages only on madvised regions</p>

<p style="margin-left:11%;"><b>--thp-never</b></p>

<p style="margin-left:22%; margin-top: 1em">Disable
transparent huge pages</p>

<p style="margin-left:11%;"><b>--thp-khugepaged-pages
&lt;pages to scan&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">Configure the
number of pages that khugepaged should scan on each pass</p>

<p style="margin-left:11%;"><b>--thp-khugepaged-scan-sleep
&lt;milliseconds&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">Configure how
many milliseconds khugepaged should wait between passes</p>


<p style="margin-left:11%;"><b>--thp-khugepages-alloc-sleep
&lt;milliseconds&gt;</b></p>

<p style="margin-left:22%; margin-top: 1em">Configure how
many milliseconds khugepaged should wait after failing to
allocate a huge page to throttle the next attempt.</p>

<p style="margin-left:11%; margin-top: 1em">The following
options affect the verbosity of libhugetlbfs. <b><br>
--verbose &lt;level&gt;, -v</b></p>

<p style="margin-left:22%; margin-top: 1em">The default
value for the verbosity level is 1 and the range of the
value can be set with --verbose from 0 to 99. The higher the
value, the more verbose the library will be. 0 is quiet and
3 will output much debugging information. The verbosity
level is increased by one each time -v is specified.</p>

<h2>SEE ALSO
<a name="SEE ALSO"></a>
</h2>



<p style="margin-left:11%; margin-top: 1em"><i>oprofile(1),
pagesize(1), libhugetlbfs(7), hugectl(8),</i></p>

<h2>AUTHORS
<a name="AUTHORS"></a>
</h2>


<p style="margin-left:11%; margin-top: 1em">libhugetlbfs
was written by various people on the libhugetlbfs-devel
mailing list.</p>
<hr>
</body>
</html>

<!-- Creator     : groff version 1.18.1.1 -->
<!-- CreationDate: Sat Nov 12 05:27:24 2016 -->
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta name="Content-Style" content="text/css">
<title></title>
</head>
<body>

<hr>

<p>RPM(8) Red Hat Linux RPM(8)</p>

<p>NAME rpm - RPM Package Manager</p>

<p>SYNOPSIS QUERYING AND VERIFYING PACKAGES: rpm
{-q|--query} [select-options] [query-options]</p>

<p>rpm {-V|--verify} [select-options] [verify-options]</p>

<p>rpm --import PUBKEY ...</p>

<p>rpm {-K|--checksig} [--nosignature] [--nodigest]
PACKAGE_FILE ...</p>

<p>INSTALLING, UPGRADING, AND REMOVING PACKAGES: rpm
{-i|--install} [install-options] PACKAGE_FILE ...</p>

<p>rpm {-U|--upgrade} [install-options] PACKAGE_FILE
...</p>

<p>rpm {-F|--freshen} [install-options] PACKAGE_FILE
...</p>

<p>rpm {-e|--erase} [--allmatches] [--nodeps] [--noscripts]
[--notriggers] [--repackage] [--test] PACKAGE_NAME ...</p>

<p>MISCELLANEOUS: rpm {--initdb|--rebuilddb}</p>

<p>rpm {--addsign|--resign} PACKAGE_FILE ...</p>

<p>rpm {--querytags|--showrc}</p>

<p>rpm {--setperms|--setugids} PACKAGE_NAME ...</p>

<p>select-options [PACKAGE_NAME] [-a,--all] [-f,--file
FILE] [-g,--group GROUP] {-p,--package PACKAGE_FILE]
[--fileid MD5] [--hdrid SHA1] [--pkgid MD5] [--tid TID]
[--querybynumber HDRNUM] [--triggeredby PACKAGE_NAME]
[--whatprovides CAPABILITY] [--whatrequires CAPABILITY]</p>

<p>query-options [--changelog] [-c,--configfiles]
[-d,--docfiles] [--dump] [--filesbypkg] [-i,--info] [--last]
[-l,--list] [--provides] [--qf,--queryformat QUERYFMT]
[-R,--requires] [--scripts] [-s,--state]
[--triggers,--triggerscripts]</p>

<p>verify-options [--nodeps] [--nofiles] [--noscripts]
[--nodigest] [--nosignature] [--nolinkto] [--nomd5]
[--nosize] [--nouser] [--nogroup] [--nomtime] [--nomode]
[--nordev]</p>

<p>install-options [--aid] [--allfiles] [--badreloc]
[--excludepath OLDPATH] [--excludedocs] [--force]
[-h,--hash] [--ignoresize] [--ignorearch] [--ignoreos]
[--includedocs] [--justdb] [--nodeps] [--nodigest]
[--nosignature] [--nosuggest] [--noorder] [--noscripts]
[--notriggers] [--oldpackage] [--percent] [--prefix NEWPATH]
[--relocate OLDPATH=NEWPATH] [--repackage] [--replacefiles]
[--replacepkgs] [--test]</p>

<p>DESCRIPTION rpm is a powerful Package Manager, which can
be used to build, install, query, verify, update, and erase
individual software packages. A pack- age consists of an
archive of files and meta-data used to install and erase the
archive files. The meta-data includes helper scripts, file
attributes, and descriptive information about the package.
Packages come in two varieties: binary packages, used to
encapsulate software to be installed, and source packages,
containing the source code and recipe necessary to produce
binary packages.</p>

<p>One of the following basic modes must be selected:
Query, Verify, Sig- nature Check, Install/Upgrade/Freshen,
Uninstall, Initialize Database, Rebuild Database, Resign,
Add Signature, Set Owners/Groups, Show Query- tags, and Show
Configuration.</p>

<p>GENERAL OPTIONS These options can be used in all the
different modes.</p>

<p>-?, --help Print a longer usage message then normal.</p>

<p>--version Print a single line containing the version
number of rpm being used.</p>

<p>--quiet Print as little as possible - normally only
error messages will be displayed.</p>

<p>-v Print verbose information - normally routine progress
messages will be displayed.</p>

<p>-vv Print lots of ugly debugging information.</p>

<p>--rcfile FILELIST Each of the files in the colon
separated FILELIST is read sequentially by rpm for
configuration information. Only the first file in the list
must exist, and tildes will be expanded to the value of
$HOME. The default FILELIST is
/usr/lib/rpm/rpmrc:/usr/lib/rpm/red-
hat/rpmrc:/etc/rpmrc:~/.rpmrc.</p>

<p>--pipe CMD Pipes the output of rpm to the command
CMD.</p>

<p>--dbpath DIRECTORY Use the database in DIRECTORY rather
than the default path /var/lib/rpm</p>

<p>--root DIRECTORY Use the file system tree rooted at
DIRECTORY for all operations. Note that this means the
database within DIRECTORY will be used for dependency checks
and any scriptlet(s) (e.g. %post if installing, or %prep if
building, a package) will be run after a chroot(2) to
DIRECTORY.</p>

<p>-D, --defineMACRO EXPR Defines MACRO with value
EXPR.</p>

<p>-E, --evalEXPR Prints macro expansion of EXPR.</p>

<p>INSTALL AND UPGRADE OPTIONS The general form of an rpm
install command is</p>

<p>rpm {-i|--install} [install-options] PACKAGE_FILE
...</p>

<p>This installs a new package.</p>

<p>The general form of an rpm upgrade command is</p>

<p>rpm {-U|--upgrade} [install-options] PACKAGE_FILE
...</p>

<p>This upgrades or installs the package currently
installed to a newer version. This is the same as install,
except all other version(s) of the package are removed after
the new package is installed.</p>

<p>rpm {-F|--freshen} [install-options] PACKAGE_FILE
...</p>

<p>This will upgrade packages, but only if an earlier
version currently exists. The PACKAGE_FILE may be specified
as an ftp or http URL, in which case the package will be
downloaded before being installed. See FTP/HTTP OPTIONS for
information on rpms internal ftp and http client
support.</p>

<p>--aid Add suggested packages to the transaction set when
needed.</p>

<p>--allfiles Installs or upgrades all the missingok files
in the package, regardless if they exist.</p>

<p>--badreloc Used with --relocate, permit relocations on
all file paths, not just those OLDPATHs included in the
binary package relocation hint(s).</p>

<p>--excludepath OLDPATH Dont install files whose name
begins with OLDPATH.</p>

<p>--excludedocs Don t install any files which are marked
as documentation (which includes man pages and texinfo
documents).</p>

<p>--force Same as using --replacepkgs, --replacefiles, and
--oldpackage.</p>

<p>-h, --hash Print 50 hash marks as the package archive is
unpacked. Use with -v|--verbose for a nicer display.</p>

<p>--ignoresize Dont check mount file systems for
sufficient disk space before installing this package.</p>

<p>--ignorearch Allow installation or upgrading even if the
architectures of the binary package and host dont match.</p>

<p>--ignoreos Allow installation or upgrading even if the
operating systems of the binary package and host dont
match.</p>

<p>--includedocs Install documentation files. This is the
default behavior.</p>

<p>--justdb Update only the database, not the
filesystem.</p>

<p>--nodigest Dont verify package or header digests when
reading.</p>

<p>--nosignature Don t verify package or header signatures
when reading.</p>

<p>--nodeps Dont do a dependency check before installing or
upgrading a package.</p>

<p>--nosuggest Don t suggest package(s) that provide a
missing dependency.</p>

<p>--noorder Dont reorder the packages for an install. The
list of packages would normally be reordered to satisfy
dependencies.</p>

<p>--noscripts</p>

<p>--nopre</p>

<p>--nopost</p>

<p>--nopreun</p>

<p>--nopostun Dont execute the scriptlet of the same name.
The --noscripts option is equivalent to</p>

<p>--nopre --nopost --nopreun --nopostun</p>

<p>and turns off the execution of the corresponding %pre,
%post, %preun, and %postun scriptlet(s).</p>

<p>--notriggers</p>

<p>--notriggerin</p>

<p>--notriggerun</p>

<p>--notriggerpostun Dont execute any trigger scriptlet of
the named type. The --notriggers option is equivalent to</p>

<p>--notriggerin --notriggerun --notriggerpostun</p>

<p>and turns off execution of the corresponding %triggerin,
%trig- gerun, and %triggerpostun scriptlet(s).</p>

<p>--oldpackage Allow an upgrade to replace a newer package
with an older one.</p>

<p>--percent Print percentages as files are unpacked from
the package archive. This is intended to make rpm easy to
run from other tools.</p>

<p>--prefix NEWPATH For relocatable binary packages,
translate all file paths that start with the installation
prefix in the package relocation hint(s) to NEWPATH.</p>

<p>--relocate OLDPATH=NEWPATH For relocatable binary
packages, translate all file paths that start with OLDPATH
in the package relocation hint(s) to NEWPATH. This option
can be used repeatedly if several OLDPATHs in the package
are to be relocated.</p>

<p>--repackage Re-package the files before erasing. The
previously installed package will be named according to the
macro %_repack- age_name_fmt and will be created in the
directory named by the macro %_repackage_dir (default value
is /var/spool/repackage).</p>

<p>--replacefiles Install the packages even if they replace
files from other, already installed, packages.</p>

<p>--replacepkgs Install the packages even if some of them
are already installed on this system.</p>

<p>--test Do not install the package, simply check for and
report poten- tial conflicts.</p>

<p>ERASE OPTIONS The general form of an rpm erase command
is</p>

<p>rpm {-e|--erase} [--allmatches] [--nodeps] [--noscripts]
[--notriggers] [--repackage] [--test] PACKAGE_NAME ...</p>

<p>The following options may also be used:</p>

<p>--allmatches Remove all versions of the package which
match PACKAGE_NAME. Normally an error is issued if
PACKAGE_NAME matches multiple packages.</p>

<p>--nodeps Dont check dependencies before uninstalling the
packages.</p>

<p>--noscripts</p>

<p>--nopreun</p>

<p>--nopostun Don t execute the scriptlet of the same name.
The --noscripts option during package erase is equivalent
to</p>

<p>--nopreun --nopostun</p>

<p>and turns off the execution of the corresponding %preun,
and %postun scriptlet(s).</p>

<p>--notriggers</p>

<p>--notriggerun</p>

<p>--notriggerpostun Don t execute any trigger scriptlet of
the named type. The --notriggers option is equivalent to</p>

<p>--notriggerun --notriggerpostun</p>

<p>and turns off execution of the corresponding %triggerun,
and %triggerpostun scriptlet(s).</p>

<p>--repackage Re-package the files before erasing. The
previously installed package will be named according to the
macro %_repack- age_name_fmt and will be created in the
directory named by the macro %_repackage_dir (default value
is /var/spool/repackage).</p>

<p>--test Dont really uninstall anything, just go through
the motions. Useful in conjunction with the -vv option for
debugging.</p>

<p>QUERY OPTIONS The general form of an rpm query command
is</p>

<p>rpm {-q|--query} [select-options] [query-options]</p>

<p>You may specify the format that package information
should be printed in. To do this, you use the</p>

<p>--qf|--queryformat QUERYFMT</p>

<p>option, followed by the QUERYFMT format string. Query
formats are mod- ified versions of the standard printf(3)
formatting. The format is made up of static strings (which
may include standard C character escapes for newlines, tabs,
and other special characters) and printf(3) type formatters.
As rpm already knows the type to print, the type specifier
must be omitted however, and replaced by the name of the
header tag to be printed, enclosed by {} characters. Tag
names are case insensitive, and the leading RPMTAG_ portion
of the tag name may be omitted as well.</p>

<p>Alternate output formats may be requested by following
the tag with :typetag. Currently, the following types are
supported:</p>

<p>:armor Wrap a public key in ASCII armor.</p>

<p>:base64 Encode binary data using base64.</p>

<p>:date Use strftime(3) &quot;%c&quot; format.</p>

<p>:day Use strftime(3) &quot;%a %b %d %Y&quot; format.</p>

<p>:depflags Format dependency flags.</p>

<p>:fflags Format file flags.</p>

<p>:hex Format in hexadecimal.</p>

<p>:octal Format in octal.</p>

<p>:perms Format file permissions.</p>

<p>:shescape Escape single quotes for use in a script.</p>

<p>:triggertype Display trigger suffix.</p>

<p>For example, to print only the names of the packages
queried, you could use %{NAME} as the format string. To
print the packages name and dis- tribution information in
two columns, you could use %-30{NAME}%{DISTRI- BUTION}. rpm
will print a list of all of the tags it knows about when it
is invoked with the --querytags argument.</p>

<p>There are two subsets of options for querying: package
selection, and information selection.</p>

<p>PACKAGE SELECTION OPTIONS: PACKAGE_NAME Query installed
package named PACKAGE_NAME.</p>

<p>-a, --all Query all installed packages.</p>

<p>-f, --file FILE Query package owning FILE.</p>

<p>--fileid MD5 Query package that contains a given file
identifier, i.e. the MD5 digest of the file contents.</p>

<p>-g, --group GROUP Query packages with the group of
GROUP.</p>

<p>--hdrid SHA1 Query package that contains a given header
identifier, i.e. the SHA1 digest of the immutable header
region.</p>

<p>-p, --package PACKAGE_FILE Query an (uninstalled)
package PACKAGE_FILE. The PACKAGE_FILE may be specified as
an ftp or http style URL, in which case the package header
will be downloaded and queried. See FTP/HTTP OPTIONS for
information on rpms internal ftp and http client support.
The PACKAGE_FILE argument(s), if not a binary package, will
be interpreted as an ASCII package manifest. Comments are
permitted, starting with a #, and each line of a package
mani- fest file may include white space separated glob
expressions, including URLs with remote glob expressions,
that will be expanded to paths that are substituted in place
of the package manifest as additional PACKAGE_FILE arguments
to the query.</p>

<p>--pkgid MD5 Query package that contains a given package
identifier, i.e. the MD5 digest of the combined header and
payload contents.</p>

<p>--querybynumber HDRNUM Query the HDRNUMth database entry
directly; this is useful only for debugging.</p>

<p>--specfile SPECFILE Parse and query SPECFILE as if it
were a package. Although not all the information (e.g. file
lists) is available, this type of query permits rpm to be
used to extract information from spec files without having
to write a specfile parser.</p>

<p>--tid TID Query package(s) that have a given TID
transaction identifier. A unix time stamp is currently used
as a transaction identifier. All package(s) installed or
erased within a single transaction have a common
identifier.</p>

<p>--triggeredby PACKAGE_NAME Query packages that are
triggered by package(s) PACKAGE_NAME.</p>

<p>--whatprovides CAPABILITY Query all packages that
provide the CAPABILITY capability.</p>

<p>--whatrequires CAPABILITY Query all packages that
requires CAPABILITY for proper function- ing.</p>

<p>PACKAGE QUERY OPTIONS: --changelog Display change
information for the package.</p>

<p>-c, --configfiles List only configuration files (implies
-l).</p>

<p>-d, --docfiles List only documentation files (implies
-l).</p>

<p>--dump Dump file information as follows (implies
-l):</p>

<p>path size mtime md5sum mode owner group isconfig isdoc
rdev symlink</p>

<p>--filesbypkg List all the files in each selected
package.</p>

<p>-i, --info Display package information, including name,
version, and description. This uses the --queryformat if one
was specified.</p>

<p>--last Orders the package listing by install time such
that the latest packages are at the top.</p>

<p>-l, --list List files in package.</p>

<p>--provides List capabilities this package provides.</p>

<p>-R, --requires List packages on which this package
depends.</p>

<p>--scripts List the package specific scriptlet(s) that
are used as part of the installation and uninstallation
processes.</p>

<p>-s, --state Display the states of files in the package
(implies -l). The state of each file is one of normal, not
installed, or replaced.</p>

<p>--triggers, --triggerscripts Display the trigger
scripts, if any, which are contained in the package.</p>

<p>VERIFY OPTIONS The general form of an rpm verify command
is</p>

<p>rpm {-V|--verify} [select-options] [verify-options]</p>

<p>Verifying a package compares information about the
installed files in the package with information about the
files taken from the package metadata stored in the rpm
database. Among other things, verifying compares the size,
MD5 sum, permissions, type, owner and group of each file.
Any discrepancies are displayed. Files that were not
installed from the package, for example, documentation files
excluded on instal- lation using the
&quot;--excludedocs&quot; option, will be silently
ignored.</p>

<p>The package selection options are the same as for
package querying (including package manifest files as
arguments). Other options unique to verify mode are:</p>

<p>--nodeps Dont verify dependencies of packages.</p>

<p>--nodigest Dont verify package or header digests when
reading.</p>

<p>--nofiles Dont verify any attributes of package
files.</p>

<p>--noscripts Don t execute the %verifyscript scriptlet
(if any).</p>

<p>--nosignature Dont verify package or header signatures
when reading.</p>

<p>--nolinkto</p>

<p>--nomd5</p>

<p>--nosize</p>

<p>--nouser</p>

<p>--nogroup</p>

<p>--nomtime</p>

<p>--nomode</p>

<p>--nordev Dont verify the corresponding file
attribute.</p>

<p>The format of the output is a string of 8 characters, a
possible attribute marker:</p>

<p>c %config configuration file. d %doc documentation file.
g %ghost file (i.e. the file contents are not included in
the package payload). l %license license file. r %readme
readme file.</p>

<p>from the package header, followed by the file name. Each
of the 8 characters denotes the result of a comparison of
attribute(s) of the file to the value of those attribute(s)
recorded in the database. A single &quot;.&quot; (period)
means the test passed, while a single &quot;?&quot;
(question mark) indicates the test could not be performed
(e.g. file permissions prevent reading). Otherwise, the
(mnemonically emBoldened) character denotes failure of the
corresponding --verify test:</p>

<p>S file Size differs M Mode differs (includes permissions
and file type) 5 MD5 sum differs D Device major/minor number
mismatch L readLink(2) path mismatch U User ownership
differs G Group ownership differs T mTime differs</p>

<p>DIGITAL SIGNATURE AND DIGEST VERIFICATION The general
forms of rpm digital signature commands are</p>

<p>rpm --import PUBKEY ...</p>

<p>rpm {--checksig} [--nosignature] [--nodigest]
PACKAGE_FILE ...</p>

<p>The --checksig option checks all the digests and
signatures contained in PACKAGE_FILE to ensure the integrity
and origin of the package. Note that signatures are now
verified whenever a package is read, and --checksig is
useful to verify all of the digests and signatures asso-
ciated with a package.</p>

<p>Digital signatures cannot be verified without a public
key. An ASCII armored public key can be added to the rpm
database using --import. An imported public key is carried
in a header, and key ring management is performed exactly
like package management. For example, all currently imported
public keys can be displayed by:</p>

<p>rpm -qa gpg-pubkey*</p>

<p>Details about a specific public key, when imported, can
be displayed by querying. Heres information about the Red
Hat GPG/DSA key:</p>

<p>rpm -qi gpg-pubkey-db42a60e</p>

<p>Finally, public keys can be erased after importing just
like packages. Heres how to remove the Red Hat GPG/DSA
key</p>

<p>rpm -e gpg-pubkey-db42a60e</p>

<p>SIGNING A PACKAGE rpm --addsign|--resign PACKAGE_FILE
...</p>

<p>Both of the --addsign and --resign options generate and
insert new sig- natures for each package PACKAGE_FILE given,
replacing any existing signatures. There are two options for
historical reasons, there is no difference in behavior
currently.</p>

<p>USING GPG TO SIGN PACKAGES In order to sign packages
using GPG, rpm must be configured to run GPG and be able to
find a key ring with the appropriate keys. By default, rpm
uses the same conventions as GPG to find key rings, namely
the $GNUPGHOME environment variable. If your key rings are
not located where GPG expects them to be, you will need to
configure the macro %_gpg_path to be the location of the GPG
key rings to use.</p>

<p>For compatibility with older versions of GPG, PGP, and
rpm, only V3 OpenPGP signature packets should be configured.
Either DSA or RSA ver- ification algorithms can be used, but
DSA is preferred.</p>

<p>If you want to be able to sign packages you create
yourself, you also need to create your own public and secret
key pair (see the GPG man- ual). You will also need to
configure the rpm macros</p>

<p>%_signature The signature type. Right now only gpg and
pgp are supported.</p>

<p>%_gpg_name The name of the &quot;user&quot; whose key
you wish to use to sign your packages.</p>

<p>For example, to be able to use GPG to sign packages as
the user &quot;John Doe &lt;jdoe@foo.com&gt;&quot; from the
key rings located in /etc/rpm/.gpg using the executable
/usr/bin/gpg you would include</p>

<p>%_signature gpg %_gpg_path /etc/rpm/.gpg %_gpg_name John
Doe &lt;jdoe@foo.com&gt; %_gpgbin /usr/bin/gpg</p>

<p>in a macro configuration file. Use /etc/rpm/macros for
per-system con- figuration and ~/.rpmmacros for per-user
configuration.</p>

<p>REBUILD DATABASE OPTIONS The general form of an rpm
rebuild database command is</p>

<p>rpm {--initdb|--rebuilddb} [-v] [--dbpath DIRECTORY]
[--root DIRECTORY]</p>

<p>Use --initdb to create a new database if one doesnt
already exist (existing database is not overwritten), use
--rebuilddb to rebuild the database indices from the
installed package headers.</p>

<p>SHOWRC The command</p>

<p>rpm --showrc</p>

<p>shows the values rpm will use for all of the options are
currently set in rpmrc and macros configuration file(s).</p>

<p>FTP/HTTP OPTIONS rpm can act as an FTP and/or HTTP
client so that packages can be queried or installed from the
internet. Package files for install, upgrade, and query
operations may be specified as an ftp or http style URL:</p>

<p>ftp://USER:PASSWORD@HOST:PORT/path/to/package.rpm</p>

<p>If the :PASSWORD portion is omitted, the password will
be prompted for (once per user/hostname pair). If both the
user and password are omit- ted, anonymous ftp is used. In
all cases, passive (PASV) ftp transfers are performed.</p>

<p>rpm allows the following options to be used with ftp
URLs:</p>

<p>--ftpproxy HOST The host HOST will be used as a proxy
server for all ftp trans- fers, which allows users to ftp
through firewall machines which use proxy systems. This
option may also be specified by config- uring the macro
%_ftpproxy.</p>

<p>--ftpport PORT The TCP PORT number to use for the ftp
connection on the proxy ftp server instead of the default
port. This option may also be specified by configuring the
macro %_ftpport.</p>

<p>rpm allows the following options to be used with http
URLs:</p>

<p>--httpproxy HOST The host HOST will be used as a proxy
server for all http trans- fers. This option may also be
specified by configuring the macro %_httpproxy.</p>

<p>--httpport PORT The TCP PORT number to use for the http
connection on the proxy http server instead of the default
port. This option may also be specified by configuring the
macro %_httpport.</p>

<p>LEGACY ISSUES Executing rpmbuild The build modes of rpm
are now resident in the /usr/bin/rpmbuild exe- cutable.
Although legacy compatibility provided by the popt aliases
below has been adequate, the compatibility is not perfect;
hence build mode compatibility through popt aliases is being
removed from rpm. Install the package containing rpmbuild
(usually rpm-build) and see rpmbuild(8) for documentation of
all the rpm build modes previously documented here in
rpm(8).</p>

<p>Add the following lines to /etc/popt if you wish to
continue invoking rpmbuild from the rpm command line:</p>

<p>rpm exec --bp rpmb -bp rpm exec --bc rpmb -bc rpm exec
--bi rpmb -bi rpm exec --bl rpmb -bl rpm exec --ba rpmb -ba
rpm exec --bb rpmb -bb rpm exec --bs rpmb -bs rpm exec --tp
rpmb -tp rpm exec --tc rpmb -tc rpm exec --ti rpmb -ti rpm
exec --tl rpmb -tl rpm exec --ta rpmb -ta rpm exec --tb rpmb
-tb rpm exec --ts rpmb -ts rpm exec --rebuild rpmb --rebuild
rpm exec --recompile rpmb --recompile rpm exec --clean rpmb
--clean rpm exec --rmsource rpmb --rmsource rpm exec
--rmspec rpmb --rmspec rpm exec --target rpmb --target rpm
exec --short-circuit rpmb --short-circuit</p>

<p>FILES rpmrc Configuration /usr/lib/rpm/rpmrc
/usr/lib/rpm/redhat/rpmrc /etc/rpmrc ~/.rpmrc</p>

<p>Macro Configuration /usr/lib/rpm/macros
/usr/lib/rpm/redhat/macros /etc/rpm/macros ~/.rpmmacros</p>

<p>Database /var/lib/rpm/Basenames
/var/lib/rpm/Conflictname /var/lib/rpm/Dirnames
/var/lib/rpm/Filemd5s /var/lib/rpm/Group
/var/lib/rpm/Installtid /var/lib/rpm/Name
/var/lib/rpm/Packages /var/lib/rpm/Providename
/var/lib/rpm/Provideversion /var/lib/rpm/Pubkeys
/var/lib/rpm/Removed /var/lib/rpm/Requirename
/var/lib/rpm/Requireversion /var/lib/rpm/Sha1header
/var/lib/rpm/Sigmd5 /var/lib/rpm/Triggername</p>

<p>Temporary /var/tmp/rpm*</p>

<p>SEE ALSO popt(3), rpm2cpio(8), rpmbuild(8),</p>

<p>rpm --help - as rpm supports customizing the options via
popt aliases its impossible to guarantee that whats
described in the manual matches whats available.</p>

<p>http://www.rpm.org/ &lt;URL:http://www.rpm.org/&gt;</p>

<p>AUTHORS Marc Ewing &lt;marc@redhat.com&gt; Jeff Johnson
&lt;jbj@redhat.com&gt; Erik Troan &lt;ewt@redhat.com&gt;</p>

<p>Red Hat, Inc. 09 June 2002 RPM(8) WGET(1) GNU Wget
WGET(1)</p>

<p>NAME Wget - The non-interactive network downloader.</p>

<p>SYNOPSIS wget [option]... [URL]...</p>

<p>DESCRIPTION GNU Wget is a free utility for
non-interactive download of files from the Web. It supports
HTTP, HTTPS, and FTP protocols, as well as retrieval through
HTTP proxies.</p>

<p>Wget is non-interactive, meaning that it can work in the
background, while the user is not logged on. This allows you
to start a retrieval and disconnect from the system, letting
Wget finish the work. By con- trast, most of the Web
browsers require constant users presence, which can be a
great hindrance when transferring a lot of data.</p>

<p>Wget can follow links in HTML and XHTML pages and create
local versions of remote web sites, fully recreating the
directory structure of the original site. This is sometimes
referred to as &quot;recursive download- ing.&quot; While
doing that, Wget respects the Robot Exclusion Standard
(/robots.txt). Wget can be instructed to convert the links
in down- loaded HTML files to the local files for offline
viewing.</p>

<p>Wget has been designed for robustness over slow or
unstable network connections; if a download fails due to a
network problem, it will keep retrying until the whole file
has been retrieved. If the server sup- ports regetting, it
will instruct the server to continue the download from where
it left off.</p>

<p>OPTIONS Option Syntax</p>

<p>Since Wget uses GNU getopt to process command-line
arguments, every option has a long form along with the short
one. Long options are more convenient to remember, but take
time to type. You may freely mix dif- ferent option styles,
or specify options after the command-line argu- ments. Thus
you may write:</p>

<p>wget -r --tries=10 http://fly.srk.fer.hr/ -o log</p>

<p>The space between the option accepting an argument and
the argument may be omitted. Instead of -o log you can write
-olog.</p>

<p>You may put several options that do not require
arguments together, like:</p>

<p>wget -drc &lt;URL&gt;</p>

<p>This is a complete equivalent of:</p>

<p>wget -d -r -c &lt;URL&gt;</p>

<p>Since the options can be specified after the arguments,
you may termi- nate them with --. So the following will try
to download URL -x, reporting failure to log:</p>

<p>wget -o log -- -x</p>

<p>The options that accept comma-separated lists all
respect the conven- tion that specifying an empty list
clears its value. This can be use- ful to clear the .wgetrc
settings. For instance, if your .wgetrc sets
&quot;exclude_directories&quot; to /cgi-bin, the following
example will first reset it, and then set it to exclude
/~nobody and /~somebody. You can also clear the lists in
.wgetrc.</p>

<p>wget -X &quot; -X /~nobody,/~somebody</p>

<p>Most options that do not accept arguments are boolean
options, so named because their state can be captured with a
yes-or-no (&quot;boolean&quot;) vari- able. For example,
--follow-ftp tells Wget to follow FTP links from HTML files
and, on the other hand, --no-glob tells it not to perform
file globbing on FTP URLs. A boolean option is either
affirmative or negative (beginning with --no). All such
options share several proper- ties.</p>

<p>Unless stated otherwise, it is assumed that the default
behavior is the opposite of what the option accomplishes.
For example, the documented existence of --follow-ftp
assumes that the default is to not follow FTP links from
HTML pages.</p>

<p>Affirmative options can be negated by prepending the
--no- to the option name; negative options can be negated by
omitting the --no- pre- fix. This might seem
superfluous---if the default for an affirmative option is to
not do something, then why provide a way to explicitly turn
it off? But the startup file may in fact change the default.
For instance, using &quot;follow_ftp = off&quot; in .wgetrc
makes Wget not follow FTP links by default, and using
--no-follow-ftp is the only way to restore the factory
default from the command line.</p>

<p>Basic Startup Options</p>

<p>-V --version Display the version of Wget.</p>

<p>-h --help Print a help message describing all of Wget s
command-line options.</p>

<p>-b --background Go to background immediately after
startup. If no output file is specified via the -o, output
is redirected to wget-log.</p>

<p>-e command --execute command Execute command as if it
were a part of .wgetrc. A command thus invoked will be
executed after the commands in .wgetrc, thus taking
precedence over them. If you need to specify more than one
wgetrc command, use multiple instances of -e.</p>

<p>Logging and Input File Options</p>

<p>-o logfile --output-file=logfile Log all messages to
logfile. The messages are normally reported to standard
error.</p>

<p>-a logfile --append-output=logfile Append to logfile.
This is the same as -o, only it appends to log- file instead
of overwriting the old log file. If logfile does not exist,
a new file is created.</p>

<p>-d --debug Turn on debug output, meaning various
information important to the developers of Wget if it does
not work properly. Your system administrator may have chosen
to compile Wget without debug sup- port, in which case -d
will not work. Please note that compiling with debug support
is always safe---Wget compiled with the debug support will
not print any debug info unless requested with -d.</p>

<p>-q --quiet Turn off Wgets output.</p>

<p>-v --verbose Turn on verbose output, with all the
available data. The default output is verbose.</p>

<p>-nv --no-verbose Turn off verbose without being
completely quiet (use -q for that), which means that error
messages and basic information still get printed.</p>

<p>-i file --input-file=file Read URLs from file. If - is
specified as file, URLs are read from the standard input.
(Use ./- to read from a file literally named -.)</p>

<p>If this function is used, no URLs need be present on the
command line. If there are URLs both on the command line and
in an input file, those on the command lines will be the
first ones to be retrieved. The file need not be an HTML
document (but no harm if it is)---it is enough if the URLs
are just listed sequentially.</p>

<p>However, if you specify --force-html, the document will
be regarded as html. In that case you may have problems with
relative links, which you can solve either by adding
&quot;&lt;base href=&quot;url&quot;&gt;&quot; to the
documents or by specifying --base=url on the command
line.</p>

<p>-F --force-html When input is read from a file, force it
to be treated as an HTML file. This enables you to retrieve
relative links from existing HTML files on your local disk,
by adding &quot;&lt;base href=&quot;url&quot;&gt;&quot; to
HTML, or using the --base command-line option.</p>

<p>-B URL --base=URL Prepends URL to relative links read
from the file specified with the -i option.</p>

<p>Download Options</p>

<p>--bind-address=ADDRESS When making client TCP/IP
connections, bind to ADDRESS on the local machine. ADDRESS
may be specified as a hostname or IP address. This option
can be useful if your machine is bound to multiple IPs.</p>

<p>-t number --tries=number Set number of retries to
number. Specify 0 or inf for infinite retrying. The default
is to retry 20 times, with the exception of fatal errors
like &quot;connection refused&quot; or &quot;not found&quot;
(404), which are not retried.</p>

<p>-O file --output-document=file The documents will not be
written to the appropriate files, but all will be
concatenated together and written to file. If - is used as
file, documents will be printed to standard output,
disabling link conversion. (Use ./- to print to a file
literally named -.)</p>

<p>Use of -O is not intended to mean simply &quot;use the
name file instead of the one in the URL;&quot; rather, it is
analogous to shell redirec- tion: wget -O file http://foo is
intended to work like wget -O - http://foo &gt; file; file
will be truncated immediately, and all downloaded content
will be written there.</p>

<p>For this reason, -N (for timestamp-checking) is not
supported in combination with -O: since file is always newly
created, it will always have a very new timestamp. A warning
will be issued if this combination is used.</p>

<p>Similarly, using -r or -p with -O may not work as you
expect: Wget won t just download the first file to file and
then download the rest to their normal names: all downloaded
content will be placed in file. This was disabled in version
1.11, but has been reinstated (with a warning) in 1.11.2, as
there are some cases where this behavior can actually have
some use.</p>

<p>Note that a combination with -k is only permitted when
downloading a single document, as in that case it will just
convert all rela- tive URIs to external ones; -k makes no
sense for multiple URIs when theyre all being downloaded to
a single file.</p>

<p>-nc --no-clobber If a file is downloaded more than once
in the same directory, Wget s behavior depends on a few
options, including -nc. In cer- tain cases, the local file
will be clobbered, or overwritten, upon repeated download.
In other cases it will be preserved.</p>

<p>When running Wget without -N, -nc, -r, or p, downloading
the same file in the same directory will result in the
original copy of file being preserved and the second copy
being named file.1. If that file is downloaded yet again,
the third copy will be named file.2, and so on. When -nc is
specified, this behavior is suppressed, and Wget will refuse
to download newer copies of file. Therefore,
&quot;&quot;no-clobber&quot;&quot; is actually a misnomer in
this mode---its not clob- bering that s prevented (as the
numeric suffixes were already pre- venting clobbering), but
rather the multiple version saving thats prevented.</p>

<p>When running Wget with -r or -p, but without -N or -nc,
re-down- loading a file will result in the new copy simply
overwriting the old. Adding -nc will prevent this behavior,
instead causing the original version to be preserved and any
newer copies on the server to be ignored.</p>

<p>When running Wget with -N, with or without -r or -p, the
decision as to whether or not to download a newer copy of a
file depends on the local and remote timestamp and size of
the file. -nc may not be specified at the same time as
-N.</p>

<p>Note that when -nc is specified, files with the suffixes
.html or .htm will be loaded from the local disk and parsed
as if they had been retrieved from the Web.</p>

<p>-c --continue Continue getting a partially-downloaded
file. This is useful when you want to finish up a download
started by a previous instance of Wget, or by another
program. For instance:</p>

<p>wget -c ftp://sunsite.doc.ic.ac.uk/ls-lR.Z</p>

<p>If there is a file named ls-lR.Z in the current
directory, Wget will assume that it is the first portion of
the remote file, and will ask the server to continue the
retrieval from an offset equal to the length of the local
file.</p>

<p>Note that you dont need to specify this option if you
just want the current invocation of Wget to retry
downloading a file should the connection be lost midway
through. This is the default behav- ior. -c only affects
resumption of downloads started prior to this invocation of
Wget, and whose local files are still sitting around.</p>

<p>Without -c, the previous example would just download the
remote file to ls-lR.Z.1, leaving the truncated ls-lR.Z file
alone.</p>

<p>Beginning with Wget 1.7, if you use -c on a non-empty
file, and it turns out that the server does not support
continued downloading, Wget will refuse to start the
download from scratch, which would effectively ruin existing
contents. If you really want the down- load to start from
scratch, remove the file.</p>

<p>Also beginning with Wget 1.7, if you use -c on a file
which is of equal size as the one on the server, Wget will
refuse to download the file and print an explanatory
message. The same happens when the file is smaller on the
server than locally (presumably because it was changed on
the server since your last download attempt)---because
&quot;continuing&quot; is not meaningful, no download
occurs.</p>

<p>On the other side of the coin, while using -c, any file
thats big- ger on the server than locally will be considered
an incomplete download and only &quot;(length(remote) -
length(local))&quot; bytes will be downloaded and tacked
onto the end of the local file. This behav- ior can be
desirable in certain cases---for instance, you can use wget
-c to download just the new portion that s been appended to
a data collection or log file.</p>

<p>However, if the file is bigger on the server because its
been changed, as opposed to just appended to, youll end up
with a gar- bled file. Wget has no way of verifying that the
local file is really a valid prefix of the remote file. You
need to be espe- cially careful of this when using -c in
conjunction with -r, since every file will be considered as
an &quot;incomplete download&quot; candi- date.</p>

<p>Another instance where youll get a garbled file if you
try to use -c is if you have a lame HTTP proxy that inserts
a &quot;transfer inter- rupted&quot; string into the local
file. In the future a &quot;rollback&quot; option may be
added to deal with this case.</p>

<p>Note that -c only works with FTP servers and with HTTP
servers that support the &quot;Range&quot; header.</p>

<p>--progress=type Select the type of the progress
indicator you wish to use. Legal indicators are
&quot;dot&quot; and &quot;bar&quot;.</p>

<p>The &quot;bar&quot; indicator is used by default. It
draws an ASCII progress bar graphics (a.k.a
&quot;thermometer&quot; display) indicating the status of
retrieval. If the output is not a TTY, the &quot;dot&quot;
bar will be used by default.</p>

<p>Use --progress=dot to switch to the &quot;dot&quot;
display. It traces the retrieval by printing dots on the
screen, each dot representing a fixed amount of downloaded
data.</p>

<p>When using the dotted retrieval, you may also set the
style by specifying the type as dot:style. Different styles
assign differ- ent meaning to one dot. With the
&quot;default&quot; style each dot repre- sents 1K, there
are ten dots in a cluster and 50 dots in a line. The
&quot;binary&quot; style has a more
&quot;computer&quot;-like orientation---8K dots, 16-dots
clusters and 48 dots per line (which makes for 384K lines).
The &quot;mega&quot; style is suitable for downloading very
large files---each dot represents 64K retrieved, there are
eight dots in a cluster, and 48 dots on each line (so each
line contains 3M).</p>

<p>Note that you can set the default style using the
&quot;progress&quot; com- mand in .wgetrc. That setting may
be overridden from the command line. The exception is that,
when the output is not a TTY, the &quot;dot&quot; progress
will be favored over &quot;bar&quot;. To force the bar out-
put, use --progress=bar:force.</p>

<p>-N --timestamping Turn on time-stamping.</p>

<p>-S --server-response Print the headers sent by HTTP
servers and responses sent by FTP servers.</p>

<p>--spider When invoked with this option, Wget will behave
as a Web spider, which means that it will not download the
pages, just check that they are there. For example, you can
use Wget to check your book- marks:</p>

<p>wget --spider --force-html -i bookmarks.html</p>

<p>This feature needs much more work for Wget to get close
to the functionality of real web spiders.</p>

<p>-T seconds --timeout=seconds Set the network timeout to
seconds seconds. This is equivalent to specifying
--dns-timeout, --connect-timeout, and --read-timeout, all at
the same time.</p>

<p>When interacting with the network, Wget can check for
timeout and abort the operation if it takes too long. This
prevents anomalies like hanging reads and infinite connects.
The only timeout enabled by default is a 900-second read
timeout. Setting a timeout to 0 disables it altogether.
Unless you know what you are doing, it is best not to change
the default timeout settings.</p>

<p>All timeout-related options accept decimal values, as
well as sub- second values. For example, 0.1 seconds is a
legal (though unwise) choice of timeout. Subsecond timeouts
are useful for checking server response times or for testing
network latency.</p>

<p>--dns-timeout=seconds Set the DNS lookup timeout to
seconds seconds. DNS lookups that dont complete within the
specified time will fail. By default, there is no timeout on
DNS lookups, other than that implemented by system
libraries.</p>

<p>--connect-timeout=seconds Set the connect timeout to
seconds seconds. TCP connections that take longer to
establish will be aborted. By default, there is no connect
timeout, other than that implemented by system
libraries.</p>

<p>--read-timeout=seconds Set the read (and write) timeout
to seconds seconds. The &quot;time&quot; of this timeout
refers to idle time: if, at any point in the download, no
data is received for more than the specified number of
seconds, reading fails and the download is restarted. This
option does not directly affect the duration of the entire
download.</p>

<p>Of course, the remote server may choose to terminate the
connection sooner than this option requires. The default
read timeout is 900 seconds.</p>

<p>--limit-rate=amount Limit the download speed to amount
bytes per second. Amount may be expressed in bytes,
kilobytes with the k suffix, or megabytes with the m suffix.
For example, --limit-rate=20k will limit the retrieval rate
to 20KB/s. This is useful when, for whatever rea- son, you
don t want Wget to consume the entire available
bandwidth.</p>

<p>This option allows the use of decimal numbers, usually
in conjunc- tion with power suffixes; for example,
--limit-rate=2.5k is a legal value.</p>

<p>Note that Wget implements the limiting by sleeping the
appropriate amount of time after a network read that took
less time than speci- fied by the rate. Eventually this
strategy causes the TCP transfer to slow down to
approximately the specified rate. However, it may take some
time for this balance to be achieved, so don t be sur-
prised if limiting the rate doesnt work well with very small
files.</p>

<p>-w seconds --wait=seconds Wait the specified number of
seconds between the retrievals. Use of this option is
recommended, as it lightens the server load by making the
requests less frequent. Instead of in seconds, the time can
be specified in minutes using the &quot;m&quot; suffix, in
hours using &quot;h&quot; suffix, or in days using
&quot;d&quot; suffix.</p>

<p>Specifying a large value for this option is useful if
the network or the destination host is down, so that Wget
can wait long enough to reasonably expect the network error
to be fixed before the retry. The waiting interval specified
by this function is influ- enced by
&quot;--random-wait&quot;, which see.</p>

<p>--waitretry=seconds If you dont want Wget to wait
between every retrieval, but only between retries of failed
downloads, you can use this option. Wget will use linear
backoff, waiting 1 second after the first failure on a given
file, then waiting 2 seconds after the second failure on
that file, up to the maximum number of seconds you specify.
There- fore, a value of 10 will actually make Wget wait up
to (1 + 2 + ... + 10) = 55 seconds per file.</p>

<p>Note that this option is turned on by default in the
global wgetrc file.</p>

<p>--random-wait Some web sites may perform log analysis to
identify retrieval pro- grams such as Wget by looking for
statistically significant simi- larities in the time between
requests. This option causes the time between requests to
vary between 0.5 and 1.5 * wait seconds, where wait was
specified using the --wait option, in order to mask Wgets
presence from such analysis.</p>

<p>A 2001 article in a publication devoted to development
on a popular consumer platform provided code to perform this
analysis on the fly. Its author suggested blocking at the
class C address level to ensure automated retrieval programs
were blocked despite changing DHCP-supplied addresses.</p>

<p>The --random-wait option was inspired by this
ill-advised recommen- dation to block many unrelated users
from a web site due to the actions of one.</p>

<p>--no-proxy Don t use proxies, even if the appropriate
*_proxy environment variable is defined.</p>

<p>-Q quota --quota=quota Specify download quota for
automatic retrievals. The value can be specified in bytes
(default), kilobytes (with k suffix), or megabytes (with m
suffix).</p>

<p>Note that quota will never affect downloading a single
file. So if you specify wget -Q10k
ftp://wuarchive.wustl.edu/ls-lR.gz, all of the ls-lR.gz will
be downloaded. The same goes even when several URLs are
specified on the command-line. However, quota is respected
when retrieving either recursively, or from an input file.
Thus you may safely type wget -Q2m -i sites---download will
be aborted when the quota is exceeded.</p>

<p>Setting quota to 0 or to inf unlimits the download
quota.</p>

<p>--no-dns-cache Turn off caching of DNS lookups.
Normally, Wget remembers the IP addresses it looked up from
DNS so it doesnt have to repeatedly contact the DNS server
for the same (typically small) set of hosts it retrieves
from. This cache exists in memory only; a new Wget run will
contact DNS again.</p>

<p>However, it has been reported that in some situations it
is not desirable to cache host names, even for the duration
of a short- running application like Wget. With this option
Wget issues a new DNS lookup (more precisely, a new call to
&quot;gethostbyname&quot; or &quot;getaddrinfo&quot;) each
time it makes a new connection. Please note that this option
will not affect caching that might be performed by the
resolving library or by an external caching layer, such as
NSCD.</p>

<p>If you dont understand exactly what this option does,
you probably wont need it.</p>

<p>--restrict-file-names=mode Change which characters found
in remote URLs may show up in local file names generated
from those URLs. Characters that are restricted by this
option are escaped, i.e. replaced with %HH, where HH is the
hexadecimal number that corresponds to the restricted
character.</p>

<p>By default, Wget escapes the characters that are not
valid as part of file names on your operating system, as
well as control charac- ters that are typically unprintable.
This option is useful for changing these defaults, either
because you are downloading to a non-native partition, or
because you want to disable escaping of the control
characters.</p>

<p>When mode is set to &quot;unix&quot;, Wget escapes the
character / and the control characters in the ranges 0--31
and 128--159. This is the default on Unix-like OSes.</p>

<p>When mode is set to &quot;windows&quot;, Wget escapes
the characters |, /, :, ?, &quot;, *, &lt;, &gt;, and the
control characters in the ranges 0--31 and 128--159. In
addition to this, Wget in Windows mode uses + instead of :
to separate host and port in local file names, and uses @
instead of ? to separate the query portion of the file name
from the rest. Therefore, a URL that would be saved as
www.xemacs.org:4300/search.pl?input=blah in Unix mode would
be saved as www.xemacs.org+4300/search.pl@input=blah in
Windows mode. This mode is the default on Windows.</p>

<p>If you append ,nocontrol to the mode, as in
unix,nocontrol, escap- ing of the control characters is also
switched off. You can use --restrict-file-names=nocontrol to
turn off escaping of control characters without affecting
the choice of the OS to use as file name restriction
mode.</p>

<p>-4 --inet4-only -6 --inet6-only Force connecting to IPv4
or IPv6 addresses. With --inet4-only or -4, Wget will only
connect to IPv4 hosts, ignoring AAAA records in DNS, and
refusing to connect to IPv6 addresses specified in URLs.
Conversely, with --inet6-only or -6, Wget will only connect
to IPv6 hosts and ignore A records and IPv4 addresses.</p>

<p>Neither options should be needed normally. By default,
an IPv6-aware Wget will use the address family specified by
the hosts DNS record. If the DNS responds with both IPv4 and
IPv6 addresses, Wget will try them in sequence until it
finds one it can connect to. (Also see
&quot;--prefer-family&quot; option described below.)</p>

<p>These options can be used to deliberately force the use
of IPv4 or IPv6 address families on dual family systems,
usually to aid debug- ging or to deal with broken network
configuration. Only one of --inet6-only and --inet4-only may
be specified at the same time. Neither option is available
in Wget compiled without IPv6 support.</p>

<p>--prefer-family=IPv4/IPv6/none When given a choice of
several addresses, connect to the addresses with specified
address family first. IPv4 addresses are preferred by
default.</p>

<p>This avoids spurious errors and connect attempts when
accessing hosts that resolve to both IPv6 and IPv4 addresses
from IPv4 net- works. For example, www.kame.net resolves to
2001:200:0:8002:203:47ff:fea5:3085 and to 203.178.141.194.
When the preferred family is &quot;IPv4&quot;, the IPv4
address is used first; when the preferred family is
&quot;IPv6&quot;, the IPv6 address is used first; if the
specified value is &quot;none&quot;, the address order
returned by DNS is used without change.</p>

<p>Unlike -4 and -6, this option doesnt inhibit access to
any address family, it only changes the order in which the
addresses are accessed. Also note that the reordering
performed by this option is stable---it doesnt affect order
of addresses of the same fam- ily. That is, the relative
order of all IPv4 addresses and of all IPv6 addresses
remains intact in all cases.</p>

<p>--retry-connrefused Consider &quot;connection
refused&quot; a transient error and try again. Normally Wget
gives up on a URL when it is unable to connect to the site
because failure to connect is taken as a sign that the
server is not running at all and that retries would not
help. This option is for mirroring unreliable sites whose
servers tend to disappear for short periods of time.</p>

<p>--user=user --password=password Specify the username
user and password password for both FTP and HTTP file
retrieval. These parameters can be overridden using the
--ftp-user and --ftp-password options for FTP connections
and the --http-user and --http-password options for HTTP
connections.</p>

<p>Directory Options</p>

<p>-nd --no-directories Do not create a hierarchy of
directories when retrieving recur- sively. With this option
turned on, all files will get saved to the current
directory, without clobbering (if a name shows up more than
once, the filenames will get extensions .n).</p>

<p>-x --force-directories The opposite of -nd---create a
hierarchy of directories, even if one would not have been
created otherwise. E.g. wget -x
http://fly.srk.fer.hr/robots.txt will save the downloaded
file to fly.srk.fer.hr/robots.txt.</p>

<p>-nH --no-host-directories Disable generation of
host-prefixed directories. By default, invoking Wget with -r
http://fly.srk.fer.hr/ will create a structure of
directories beginning with fly.srk.fer.hr/. This option
disables such behavior.</p>

<p>--protocol-directories Use the protocol name as a
directory component of local file names. For example, with
this option, wget -r http://host will save to http/host/...
rather than just to host/....</p>

<p>--cut-dirs=number Ignore number directory components.
This is useful for getting a fine-grained control over the
directory where recursive retrieval will be saved.</p>

<p>Take, for example, the directory at
ftp://ftp.xemacs.org/pub/xemacs/. If you retrieve it with
-r, it will be saved locally under
ftp.xemacs.org/pub/xemacs/. While the -nH option can remove
the ftp.xemacs.org/ part, you are still stuck with
pub/xemacs. This is where --cut-dirs comes in handy; it
makes Wget not &quot;see&quot; number remote directory
components. Here are sev- eral examples of how --cut-dirs
option works.</p>

<p>No options -&gt; ftp.xemacs.org/pub/xemacs/ -nH -&gt;
pub/xemacs/ -nH --cut-dirs=1 -&gt; xemacs/ -nH --cut-dirs=2
-&gt; .</p>

<p>--cut-dirs=1 -&gt; ftp.xemacs.org/xemacs/ ...</p>

<p>If you just want to get rid of the directory structure,
this option is similar to a combination of -nd and -P.
However, unlike -nd, --cut-dirs does not lose with
subdirectories---for instance, with -nH --cut-dirs=1, a
beta/ subdirectory will be placed to xemacs/beta, as one
would expect.</p>

<p>-P prefix --directory-prefix=prefix Set directory prefix
to prefix. The directory prefix is the direc- tory where all
other files and subdirectories will be saved to, i.e. the
top of the retrieval tree. The default is . (the current
directory).</p>

<p>HTTP Options</p>

<p>-E --html-extension If a file of type
application/xhtml+xml or text/html is downloaded and the URL
does not end with the regexp .[Hh][Tt][Mm][Ll]?, this option
will cause the suffix .html to be appended to the local
filename. This is useful, for instance, when youre mirroring
a remote site that uses .asp pages, but you want the
mirrored pages to be viewable on your stock Apache server.
Another good use for this is when you re downloading
CGI-generated materials. A URL like
http://site.com/article.cgi?25 will be saved as arti-
cle.cgi?25.html.</p>

<p>Note that filenames changed in this way will be
re-downloaded every time you re-mirror a site, because Wget
cant tell that the local X.html file corresponds to remote
URL X (since it doesnt yet know that the URL produces output
of type text/html or applica- tion/xhtml+xml. To prevent
this re-downloading, you must use -k and -K so that the
original version of the file will be saved as X.orig.</p>

<p>--http-user=user --http-password=password Specify the
username user and password password on an HTTP server.
According to the type of the challenge, Wget will encode
them using either the &quot;basic&quot; (insecure), the
&quot;digest&quot;, or the Windows &quot;NTLM&quot;
authentication scheme.</p>

<p>Another way to specify username and password is in the
URL itself. Either method reveals your password to anyone
who bothers to run &quot;ps&quot;. To prevent the passwords
from being seen, store them in .wgetrc or .netrc, and make
sure to protect those files from other users with
&quot;chmod&quot;. If the passwords are really important, do
not leave them lying in those files either---edit the files
and delete them after Wget has started the download.</p>

<p>--no-cache Disable server-side cache. In this case, Wget
will send the remote server an appropriate directive
(Pragma: no-cache) to get the file from the remote service,
rather than returning the cached version. This is especially
useful for retrieving and flushing out-of-date documents on
proxy servers.</p>

<p>Caching is allowed by default.</p>

<p>--no-cookies Disable the use of cookies. Cookies are a
mechanism for maintain- ing server-side state. The server
sends the client a cookie using the &quot;Set-Cookie&quot;
header, and the client responds with the same cookie upon
further requests. Since cookies allow the server own- ers to
keep track of visitors and for sites to exchange this infor-
mation, some consider them a breach of privacy. The default
is to use cookies; however, storing cookies is not on by
default.</p>

<p>--load-cookies file Load cookies from file before the
first HTTP retrieval. file is a textual file in the format
originally used by Netscape s cook- ies.txt file.</p>

<p>You will typically use this option when mirroring sites
that require that you be logged in to access some or all of
their con- tent. The login process typically works by the
web server issuing an HTTP cookie upon receiving and
verifying your credentials. The cookie is then resent by the
browser when accessing that part of the site, and so proves
your identity.</p>

<p>Mirroring such a site requires Wget to send the same
cookies your browser sends when communicating with the site.
This is achieved by --load-cookies---simply point Wget to
the location of the cook- ies.txt file, and it will send the
same cookies your browser would send in the same situation.
Different browsers keep textual cookie files in different
locations:</p>

<p>Netscape 4.x. The cookies are in
~/.netscape/cookies.txt.</p>

<p>Mozilla and Netscape 6.x. Mozillas cookie file is also
named cookies.txt, located some- where under ~/.mozilla, in
the directory of your profile. The full path usually ends up
looking somewhat like
~/.mozilla/default/some-weird-string/cookies.txt.</p>

<p>Internet Explorer. You can produce a cookie file Wget
can use by using the File menu, Import and Export, Export
Cookies. This has been tested with Internet Explorer 5; it
is not guaranteed to work with earlier versions.</p>

<p>Other browsers. If you are using a different browser to
create your cookies, --load-cookies will only work if you
can locate or produce a cookie file in the Netscape format
that Wget expects.</p>

<p>If you cannot use --load-cookies, there might still be
an alterna- tive. If your browser supports a &quot;cookie
manager&quot;, you can use it to view the cookies used when
accessing the site youre mirroring. Write down the name and
value of the cookie, and manually instruct Wget to send
those cookies, bypassing the &quot;official&quot; cookie
sup- port:</p>

<p>wget --no-cookies --header &quot;Cookie:
&lt;name&gt;=&lt;value&gt;&quot;</p>

<p>--save-cookies file Save cookies to file before exiting.
This will not save cookies that have expired or that have no
expiry time (so-called &quot;session cookies&quot;), but
also see --keep-session-cookies.</p>

<p>--keep-session-cookies When specified, causes
--save-cookies to also save session cookies. Session cookies
are normally not saved because they are meant to be kept in
memory and forgotten when you exit the browser. Saving them
is useful on sites that require you to log in or to visit
the home page before you can access some pages. With this
option, mul- tiple Wget runs are considered a single browser
session as far as the site is concerned.</p>

<p>Since the cookie file format does not normally carry
session cook- ies, Wget marks them with an expiry timestamp
of 0. Wgets --load-cookies recognizes those as session
cookies, but it might confuse other browsers. Also note that
cookies so loaded will be treated as other session cookies,
which means that if you want --save-cookies to preserve them
again, you must use --keep-ses- sion-cookies again.</p>

<p>--ignore-length Unfortunately, some HTTP servers (CGI
programs, to be more precise) send out bogus
&quot;Content-Length&quot; headers, which makes Wget go
wild, as it thinks not all the document was retrieved. You
can spot this syndrome if Wget retries getting the same
document again and again, each time claiming that the
(otherwise normal) connection has closed on the very same
byte.</p>

<p>With this option, Wget will ignore the
&quot;Content-Length&quot; header---as if it never
existed.</p>

<p>--header=header-line Send header-line along with the
rest of the headers in each HTTP request. The supplied
header is sent as-is, which means it must contain name and
value separated by colon, and must not contain newlines.</p>

<p>You may define more than one additional header by
specifying --header more than once.</p>

<p>wget --header=&rsquo;Accept-Charset: iso-8859-2&rsquo;
--header=&rsquo;Accept-Language: hr&rsquo;
http://fly.srk.fer.hr/</p>

<p>Specification of an empty string as the header value
will clear all previous user-defined headers.</p>

<p>As of Wget 1.10, this option can be used to override
headers other- wise generated automatically. This example
instructs Wget to con- nect to localhost, but to specify
foo.bar in the &quot;Host&quot; header:</p>

<p>wget --header=&quot;Host: foo.bar&quot;
http://localhost/</p>

<p>In versions of Wget prior to 1.10 such use of --header
caused send- ing of duplicate headers.</p>

<p>--max-redirect=number Specifies the maximum number of
redirections to follow for a resource. The default is 20,
which is usually far more than neces- sary. However, on
those occasions where you want to allow more (or fewer),
this is the option to use.</p>

<p>--proxy-user=user --proxy-password=password Specify the
username user and password password for authentication on a
proxy server. Wget will encode them using the
&quot;basic&quot; authen- tication scheme.</p>

<p>Security considerations similar to those with
--http-password per- tain here as well.</p>

<p>--referer=url Include Referer: url header in HTTP
request. Useful for retriev- ing documents with server-side
processing that assume they are always being retrieved by
interactive web browsers and only come out properly when
Referer is set to one of the pages that point to them.</p>

<p>--save-headers Save the headers sent by the HTTP server
to the file, preceding the actual contents, with an empty
line as the separator.</p>

<p>-U agent-string --user-agent=agent-string Identify as
agent-string to the HTTP server.</p>

<p>The HTTP protocol allows the clients to identify
themselves using a &quot;User-Agent&quot; header field. This
enables distinguishing the WWW software, usually for
statistical purposes or for tracing of proto- col
violations. Wget normally identifies as Wget/version,
version being the current version number of Wget.</p>

<p>However, some sites have been known to impose the policy
of tailor- ing the output according to the
&quot;User-Agent&quot;-supplied information. While this is
not such a bad idea in theory, it has been abused by servers
denying information to clients other than (historically)
Netscape or, more frequently, Microsoft Internet Explorer.
This option allows you to change the &quot;User-Agent&quot;
line issued by Wget. Use of this option is discouraged,
unless you really know what you are doing.</p>

<p>Specifying empty user agent with
--user-agent=&quot;&quot; instructs Wget not to send the
&quot;User-Agent&quot; header in HTTP requests.</p>

<p>--post-data=string --post-file=file Use POST as the
method for all HTTP requests and send the specified data in
the request body. &quot;--post-data&quot; sends string as
data, whereas &quot;--post-file&quot; sends the contents of
file. Other than that, they work in exactly the same
way.</p>

<p>Please be aware that Wget needs to know the size of the
POST data in advance. Therefore the argument to
&quot;--post-file&quot; must be a reg- ular file; specifying
a FIFO or something like /dev/stdin wont work. It s not
quite clear how to work around this limitation inherent in
HTTP/1.0. Although HTTP/1.1 introduces chunked trans- fer
that doesnt require knowing the request length in advance, a
client cant use chunked unless it knows its talking to an
HTTP/1.1 server. And it can t know that until it receives a
response, which in turn requires the request to have been
completed -- a chicken-and-egg problem.</p>

<p>Note: if Wget is redirected after the POST request is
completed, it will not send the POST data to the redirected
URL. This is because URLs that process POST often respond
with a redirection to a regu- lar page, which does not
desire or accept POST. It is not com- pletely clear that
this behavior is optimal; if it doesnt work out, it might be
changed in the future.</p>

<p>This example shows how to log to a server using POST and
then pro- ceed to download the desired pages, presumably
only accessible to authorized users:</p>

<p># Log in to the server. This can be done only once. wget
--save-cookies cookies.txt --post-data
&rsquo;user=foo&amp;password=bar&rsquo;
http://server.com/auth.php</p>

<p># Now grab the page or pages we care about. wget
--load-cookies cookies.txt -p
http://server.com/interesting/article.php</p>

<p>If the server is using session cookies to track user
authentica- tion, the above will not work because
--save-cookies will not save them (and neither will
browsers) and the cookies.txt file will be empty. In that
case use --keep-session-cookies along with --save-cookies to
force saving of session cookies.</p>

<p>--content-disposition If this is set to on, experimental
(not fully-functional) support for
&quot;Content-Disposition&quot; headers is enabled. This can
currently result in extra round-trips to the server for a
&quot;HEAD&quot; request, and is known to suffer from a few
bugs, which is why it is not cur- rently enabled by
default.</p>

<p>This option is useful for some file-downloading CGI
programs that use &quot;Content-Disposition&quot; headers to
describe what the name of a downloaded file should be.</p>

<p>--auth-no-challenge If this option is given, Wget will
send Basic HTTP authentication information (plaintext
username and password) for all requests, just like Wget
1.10.2 and prior did by default.</p>

<p>Use of this option is not recommended, and is intended
only to sup- port some few obscure servers, which never send
HTTP authentication challenges, but accept unsolicited auth
info, say, in addition to form-based authentication.</p>

<p>HTTPS (SSL/TLS) Options</p>

<p>To support encrypted HTTP (HTTPS) downloads, Wget must
be compiled with an external SSL library, currently OpenSSL.
If Wget is compiled with- out SSL support, none of these
options are available.</p>

<p>--secure-protocol=protocol Choose the secure protocol to
be used. Legal values are auto, SSLv2, SSLv3, and TLSv1. If
auto is used, the SSL library is given the liberty of
choosing the appropriate protocol automatically, which is
achieved by sending an SSLv2 greeting and announcing sup-
port for SSLv3 and TLSv1. This is the default.</p>

<p>Specifying SSLv2, SSLv3, or TLSv1 forces the use of the
correspond- ing protocol. This is useful when talking to old
and buggy SSL server implementations that make it hard for
OpenSSL to choose the correct protocol version. Fortunately,
such servers are quite rare.</p>

<p>--no-check-certificate Don t check the server
certificate against the available certifi- cate authorities.
Also dont require the URL host name to match the common name
presented by the certificate.</p>

<p>As of Wget 1.10, the default is to verify the servers
certificate against the recognized certificate authorities,
breaking the SSL handshake and aborting the download if the
verification fails. Although this provides more secure
downloads, it does break inter- operability with some sites
that worked with previous Wget ver- sions, particularly
those using self-signed, expired, or otherwise invalid
certificates. This option forces an &quot;insecure&quot;
mode of operation that turns the certificate verification
errors into warn- ings and allows you to proceed.</p>

<p>If you encounter &quot;certificate verification&quot;
errors or ones saying that &quot;common name doesnt match
requested host name&quot;, you can use this option to bypass
the verification and proceed with the down- load. Only use
this option if you are otherwise convinced of the site_s
authenticity, or if you really dont care about the validity
of its certificate. It is almost always a bad idea not to
check the certificates when transmitting confidential or
important data.</p>

<p>--certificate=file Use the client certificate stored in
file. This is needed for servers that are configured to
require certificates from the clients that connect to them.
Normally a certificate is not required and this switch is
optional.</p>

<p>--certificate-type=type Specify the type of the client
certificate. Legal values are PEM (assumed by default) and
DER, also known as ASN1.</p>

<p>--private-key=file Read the private key from file. This
allows you to provide the private key in a file separate
from the certificate.</p>

<p>--private-key-type=type Specify the type of the private
key. Accepted values are PEM (the default) and DER.</p>

<p>--ca-certificate=file Use file as the file with the
bundle of certificate authorities (&quot;CA&quot;) to verify
the peers. The certificates must be in PEM for- mat.</p>

<p>Without this option Wget looks for CA certificates at
the system- specified locations, chosen at OpenSSL
installation time.</p>

<p>--ca-directory=directory Specifies directory containing
CA certificates in PEM format. Each file contains one CA
certificate, and the file name is based on a hash value
derived from the certificate. This is achieved by pro-
cessing a certificate directory with the
&quot;c_rehash&quot; utility sup- plied with OpenSSL. Using
--ca-directory is more efficient than --ca-certificate when
many certificates are installed because it allows Wget to
fetch certificates on demand.</p>

<p>Without this option Wget looks for CA certificates at
the system- specified locations, chosen at OpenSSL
installation time.</p>

<p>--random-file=file Use file as the source of random data
for seeding the pseudo-random number generator on systems
without /dev/random.</p>

<p>On such systems the SSL library needs an external source
of random- ness to initialize. Randomness may be provided by
EGD (see --egd-file below) or read from an external source
specified by the user. If this option is not specified, Wget
looks for random data in $RANDFILE or, if that is unset, in
$HOME/.rnd. If none of those are available, it is likely
that SSL encryption will not be usable.</p>

<p>If you re getting the &quot;Could not seed OpenSSL PRNG;
disabling SSL.&quot; error, you should provide random data
using some of the methods described above.</p>

<p>--egd-file=file Use file as the EGD socket. EGD stands
for Entropy Gathering Dae- mon, a user-space program that
collects data from various unpre- dictable system sources
and makes it available to other programs that might need it.
Encryption software, such as the SSL library, needs sources
of non-repeating randomness to seed the random number
generator used to produce cryptographically strong keys.</p>

<p>OpenSSL allows the user to specify his own source of
entropy using the &quot;RAND_FILE&quot; environment
variable. If this variable is unset, or if the specified
file does not produce enough randomness, OpenSSL will read
random data from EGD socket specified using this option.</p>

<p>If this option is not specified (and the equivalent
startup command is not used), EGD is never contacted. EGD is
not needed on modern Unix systems that support
/dev/random.</p>

<p>FTP Options</p>

<p>--ftp-user=user --ftp-password=password Specify the
username user and password password on an FTP server.
Without this, or the corresponding startup option, the
password defaults to -wget@, normally used for anonymous
FTP.</p>

<p>Another way to specify username and password is in the
URL itself. Either method reveals your password to anyone
who bothers to run &quot;ps&quot;. To prevent the passwords
from being seen, store them in .wgetrc or .netrc, and make
sure to protect those files from other users with
&quot;chmod&quot;. If the passwords are really important, do
not leave them lying in those files either---edit the files
and delete them after Wget has started the download.</p>

<p>--no-remove-listing Don t remove the temporary .listing
files generated by FTP retrievals. Normally, these files
contain the raw directory list- ings received from FTP
servers. Not removing them can be useful for debugging
purposes, or when you want to be able to easily check on the
contents of remote server directories (e.g. to verify that a
mirror youre running is complete).</p>

<p>Note that even though Wget writes to a known filename
for this file, this is not a security hole in the scenario
of a user making .listing a symbolic link to /etc/passwd or
something and asking &quot;root&quot; to run Wget in his or
her directory. Depending on the options used, either Wget
will refuse to write to .listing, making the
globbing/recursion/time-stamping operation fail, or the sym-
bolic link will be deleted and replaced with the actual
.listing file, or the listing will be written to a
.listing.number file.</p>

<p>Even though this situation isn t a problem, though,
&quot;root&quot; should never run Wget in a non-trusted
users directory. A user could do something as simple as
linking index.html to /etc/passwd and asking
&quot;root&quot; to run Wget with -N or -r so the file will
be overwritten.</p>

<p>--no-glob Turn off FTP globbing. Globbing refers to the
use of shell-like special characters (wildcards), like *, ?,
[ and ] to retrieve more than one file from the same
directory at once, like:</p>

<p>wget ftp://gnjilux.srk.fer.hr/*.msg</p>

<p>By default, globbing will be turned on if the URL
contains a glob- bing character. This option may be used to
turn globbing on or off permanently.</p>

<p>You may have to quote the URL to protect it from being
expanded by your shell. Globbing makes Wget look for a
directory listing, which is system-specific. This is why it
currently works only with Unix FTP servers (and the ones
emulating Unix &quot;ls&quot; output).</p>

<p>--no-passive-ftp Disable the use of the passive FTP
transfer mode. Passive FTP man- dates that the client
connect to the server to establish the data connection
rather than the other way around.</p>

<p>If the machine is connected to the Internet directly,
both passive and active FTP should work equally well. Behind
most firewall and NAT configurations passive FTP has a
better chance of working. However, in some rare firewall
configurations, active FTP actually works when passive FTP
doesnt. If you suspect this to be the case, use this option,
or set &quot;passive_ftp=off&quot; in your init file.</p>

<p>--retr-symlinks Usually, when retrieving FTP directories
recursively and a symbolic link is encountered, the
linked-to file is not downloaded. Instead, a matching
symbolic link is created on the local filesys- tem. The
pointed-to file will not be downloaded unless this recur-
sive retrieval would have encountered it separately and
downloaded it anyway.</p>

<p>When --retr-symlinks is specified, however, symbolic
links are tra- versed and the pointed-to files are
retrieved. At this time, this option does not cause Wget to
traverse symlinks to directories and recurse through them,
but in the future it should be enhanced to do this.</p>

<p>Note that when retrieving a file (not a directory)
because it was specified on the command-line, rather than
because it was recursed to, this option has no effect.
Symbolic links are always traversed in this case.</p>

<p>--no-http-keep-alive Turn off the &quot;keep-alive&quot;
feature for HTTP downloads. Normally, Wget asks the server
to keep the connection open so that, when you download more
than one document from the same server, they get transferred
over the same TCP connection. This saves time and at the
same time reduces the load on the server.</p>

<p>This option is useful when, for some reason, persistent
(keep-alive) connections don t work for you, for example due
to a server bug or due to the inability of server-side
scripts to cope with the connections.</p>

<p>Recursive Retrieval Options</p>

<p>-r --recursive Turn on recursive retrieving.</p>

<p>-l depth --level=depth Specify recursion maximum depth
level depth. The default maximum depth is 5.</p>

<p>--delete-after This option tells Wget to delete every
single file it downloads, after having done so. It is useful
for pre-fetching popular pages through a proxy, e.g.:</p>

<p>wget -r -nd --delete-after
http://whatever.com/~popular/page/</p>

<p>The -r option is to retrieve recursively, and -nd to not
create directories.</p>

<p>Note that --delete-after deletes files on the local
machine. It does not issue the DELE command to remote FTP
sites, for instance. Also note that when --delete-after is
specified, --convert-links is ignored, so .orig files are
simply not created in the first place.</p>

<p>-k --convert-links After the download is complete,
convert the links in the document to make them suitable for
local viewing. This affects not only the visible hyperlinks,
but any part of the document that links to external content,
such as embedded images, links to style sheets, hyperlinks
to non-HTML content, etc.</p>

<p>Each link will be changed in one of the two ways:</p>

<p>* The links to files that have been downloaded by Wget
will be changed to refer to the file they point to as a
relative link.</p>

<p>Example: if the downloaded file /foo/doc.html links to
/bar/img.gif, also downloaded, then the link in doc.html
will be modified to point to ../bar/img.gif. This kind of
transfor- mation works reliably for arbitrary combinations
of directo- ries.</p>

<p>* The links to files that have not been downloaded by
Wget will be changed to include host name and absolute path
of the loca- tion they point to.</p>

<p>Example: if the downloaded file /foo/doc.html links to
/bar/img.gif (or to ../bar/img.gif), then the link in
doc.html will be modified to point to
http://hostname/bar/img.gif.</p>

<p>Because of this, local browsing works reliably: if a
linked file was downloaded, the link will refer to its local
name; if it was not downloaded, the link will refer to its
full Internet address rather than presenting a broken link.
The fact that the former links are converted to relative
links ensures that you can move the downloaded hierarchy to
another directory.</p>

<p>Note that only at the end of the download can Wget know
which links have been downloaded. Because of that, the work
done by -k will be performed at the end of all the
downloads.</p>

<p>-K --backup-converted When converting a file, back up
the original version with a .orig suffix. Affects the
behavior of -N.</p>

<p>-m --mirror Turn on options suitable for mirroring. This
option turns on recursion and time-stamping, sets infinite
recursion depth and keeps FTP directory listings. It is
currently equivalent to -r -N -l inf
--no-remove-listing.</p>

<p>-p --page-requisites This option causes Wget to download
all the files that are neces- sary to properly display a
given HTML page. This includes such things as inlined
images, sounds, and referenced stylesheets.</p>

<p>Ordinarily, when downloading a single HTML page, any
requisite doc- uments that may be needed to display it
properly are not down- loaded. Using -r together with -l can
help, but since Wget does not ordinarily distinguish between
external and inlined documents, one is generally left with
&quot;leaf documents&quot; that are missing their
requisites.</p>

<p>For instance, say document 1.html contains an
&quot;&lt;IMG&gt;&quot; tag referenc- ing 1.gif and an
&quot;&lt;A&gt;&quot; tag pointing to external document
2.html. Say that 2.html is similar but that its image is
2.gif and it links to 3.html. Say this continues up to some
arbitrarily high number.</p>

<p>If one executes the command:</p>

<p>wget -r -l 2 http://&lt;site&gt;/1.html</p>

<p>then 1.html, 1.gif, 2.html, 2.gif, and 3.html will be
downloaded. As you can see, 3.html is without its requisite
3.gif because Wget is simply counting the number of hops (up
to 2) away from 1.html in order to determine where to stop
the recursion. However, with this command:</p>

<p>wget -r -l 2 -p http://&lt;site&gt;/1.html</p>

<p>all the above files and 3.htmls requisite 3.gif will be
down- loaded. Similarly,</p>

<p>wget -r -l 1 -p http://&lt;site&gt;/1.html</p>

<p>will cause 1.html, 1.gif, 2.html, and 2.gif to be
downloaded. One might think that:</p>

<p>wget -r -l 0 -p http://&lt;site&gt;/1.html</p>

<p>would download just 1.html and 1.gif, but unfortunately
this is not the case, because -l 0 is equivalent to -l
inf---that is, infinite recursion. To download a single HTML
page (or a handful of them, all specified on the
command-line or in a -i URL input file) and its (or their)
requisites, simply leave off -r and -l:</p>

<p>wget -p http://&lt;site&gt;/1.html</p>

<p>Note that Wget will behave as if -r had been specified,
but only that single page and its requisites will be
downloaded. Links from that page to external documents will
not be followed. Actually, to download a single page and all
its requisites (even if they exist on separate websites),
and make sure the lot displays properly locally, this author
likes to use a few options in addition to -p:</p>

<p>wget -E -H -k -K -p
http://&lt;site&gt;/&lt;document&gt;</p>

<p>To finish off this topic, its worth knowing that Wgets
idea of an external document link is any URL specified in an
&quot;&lt;A&gt;&quot; tag, an &quot;&lt;AREA&gt;&quot; tag,
or a &quot;&lt;LINK&gt;&quot; tag other than &quot;&lt;LINK
REL=&quot;stylesheet&quot;&gt;&quot;.</p>

<p>--strict-comments Turn on strict parsing of HTML
comments. The default is to termi- nate comments at the
first occurrence of --&gt;.</p>

<p>According to specifications, HTML comments are expressed
as SGML declarations. Declaration is special markup that
begins with &lt;! and ends with &gt;, such as &lt;!DOCTYPE
...&gt;, that may contain comments between a pair of --
delimiters. HTML comments are &quot;empty declara-
tions&quot;, SGML declarations without any non-comment text.
Therefore, &lt;!--foo--&gt; is a valid comment, and so is
&lt;!--one-- --two--&gt;, but &lt;!--1--2--&gt; is not.</p>

<p>On the other hand, most HTML writers don t perceive
comments as anything other than text delimited with &lt;!--
and --&gt;, which is not quite the same. For example,
something like &lt;!------------&gt; works as a valid
comment as long as the number of dashes is a multiple of
four (!). If not, the comment technically lasts until the
next --, which may be at the other end of the document.
Because of this, many popular browsers completely ignore the
specification and implement what users have come to expect:
comments delimited with &lt;!-- and --&gt;.</p>

<p>Until version 1.9, Wget interpreted comments strictly,
which resulted in missing links in many web pages that
displayed fine in browsers, but had the misfortune of
containing non-compliant com- ments. Beginning with version
1.9, Wget has joined the ranks of clients that implements
&quot;naive&quot; comments, terminating each comment at the
first occurrence of --&gt;.</p>

<p>If, for whatever reason, you want strict comment
parsing, use this option to turn it on.</p>

<p>Recursive Accept/Reject Options</p>

<p>-A acclist --accept acclist -R rejlist --reject rejlist
Specify comma-separated lists of file name suffixes or
patterns to accept or reject. Note that if any of the
wildcard characters, *, ?, [ or ], appear in an element of
acclist or rejlist, it will be treated as a pattern, rather
than a suffix.</p>

<p>-D domain-list --domains=domain-list Set domains to be
followed. domain-list is a comma-separated list of domains.
Note that it does not turn on -H.</p>

<p>--exclude-domains domain-list Specify the domains that
are not to be followed..</p>

<p>--follow-ftp Follow FTP links from HTML documents.
Without this option, Wget will ignore all the FTP links.</p>

<p>--follow-tags=list Wget has an internal table of HTML
tag / attribute pairs that it considers when looking for
linked documents during a recursive retrieval. If a user
wants only a subset of those tags to be con- sidered,
however, he or she should be specify such tags in a comma-
separated list with this option.</p>

<p>--ignore-tags=list This is the opposite of the
--follow-tags option. To skip certain HTML tags when
recursively looking for documents to download, specify them
in a comma-separated list.</p>

<p>In the past, this option was the best bet for
downloading a single page and its requisites, using a
command-line like:</p>

<p>wget --ignore-tags=a,area -H -k -K -r
http://&lt;site&gt;/&lt;document&gt;</p>

<p>However, the author of this option came across a page
with tags like &quot;&lt;LINK REL=&quot;home&quot;
HREF=&quot;/&quot;&gt;&quot; and came to the realization
that specifying tags to ignore was not enough. One cant just
tell Wget to ignore &quot;&lt;LINK&gt;&quot;, because then
stylesheets will not be down- loaded. Now the best bet for
downloading a single page and its requisites is the
dedicated --page-requisites option.</p>

<p>--ignore-case Ignore case when matching files and
directories. This influences the behavior of -R, -A, -I, and
-X options, as well as globbing implemented when downloading
from FTP sites. For example, with this option, -A *.txt will
match file1.txt, but also file2.TXT, file3.TxT, and so
on.</p>

<p>-H --span-hosts Enable spanning across hosts when doing
recursive retrieving.</p>

<p>-L --relative Follow relative links only. Useful for
retrieving a specific home page without any distractions,
not even those from the same hosts.</p>

<p>-I list --include-directories=list Specify a
comma-separated list of directories you wish to follow when
downloading. Elements of list may contain wildcards.</p>

<p>-X list --exclude-directories=list Specify a
comma-separated list of directories you wish to exclude from
download. Elements of list may contain wildcards.</p>

<p>-np --no-parent Do not ever ascend to the parent
directory when retrieving recur- sively. This is a useful
option, since it guarantees that only the files below a
certain hierarchy will be downloaded.</p>

<p>FILES /etc/wgetrc Default location of the global startup
file.</p>

<p>.wgetrc User startup file.</p>

<p>BUGS You are welcome to submit bug reports via the GNU
Wget bug tracker (see
&lt;http://wget.addictivecode.org/BugTracker&gt;).</p>

<p>Before actually submitting a bug report, please try to
follow a few simple guidelines.</p>

<p>1. Please try to ascertain that the behavior you see
really is a bug. If Wget crashes, its a bug. If Wget does
not behave as docu- mented, its a bug. If things work
strange, but you are not sure about the way they are
supposed to work, it might well be a bug, but you might want
to double-check the documentation and the mail- ing
lists.</p>

<p>2. Try to repeat the bug in as simple circumstances as
possible. E.g. if Wget crashes while downloading wget -rl0
-kKE -t5 --no-proxy http://yoyodyne.com -o /tmp/log, you
should try to see if the crash is repeatable, and if will
occur with a simpler set of options. You might even try to
start the download at the page where the crash occurred to
see if that page somehow triggered the crash.</p>

<p>Also, while I will probably be interested to know the
contents of your .wgetrc file, just dumping it into the
debug message is proba- bly a bad idea. Instead, you should
first try to see if the bug repeats with .wgetrc moved out
of the way. Only if it turns out that .wgetrc settings
affect the bug, mail me the relevant parts of the file.</p>

<p>3. Please start Wget with -d option and send us the
resulting output (or relevant parts thereof). If Wget was
compiled without debug support, recompile it---it is much
easier to trace bugs with debug support on.</p>

<p>Note: please make sure to remove any potentially
sensitive informa- tion from the debug log before sending it
to the bug address. The &quot;-d&quot; won t go out of its
way to collect sensitive information, but the log will
contain a fairly complete transcript of Wgets commu-
nication with the server, which may include passwords and
pieces of downloaded data. Since the bug address is
publically archived, you may assume that all bug reports are
visible to the public.</p>

<p>4. If Wget has crashed, try to run it in a debugger,
e.g. &quot;gdb which wget core&quot; and type
&quot;where&quot; to get the backtrace. This may not work if
the system administrator has disabled core files, but it is
safe to try.</p>

<p>SEE ALSO This is not the complete manual for GNU Wget.
For more complete infor- mation, including more detailed
explanations of some of the options, and a number of
commands available for use with .wgetrc files and the -e
option, see the GNU Info entry for wget.</p>

<p>AUTHOR Originally written by Hrvoje Niksic
&lt;hniksic@xemacs.org&gt;. Currently maintained by Micah
Cowan &lt;micah@cowan.name&gt;.</p>

<p>COPYRIGHT Copyright (c) 1996, 1997, 1998, 1999, 2000,
2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008 Free Software
Foundation, Inc.</p>

<p>Permission is granted to copy, distribute and/or modify
this document under the terms of the GNU Free Documentation
License, Version 1.2 or any later version published by the
Free Software Foundation; with no Invariant Sections, no
Front-Cover Texts, and no Back-Cover Texts. A copy of the
license is included in the section entitled &quot;GNU Free
Docu- mentation License&quot;.</p>

<p>GNU Wget 1.11.4 2008-06-29 WGET(1)</p>
<hr>
</body>
</html>
